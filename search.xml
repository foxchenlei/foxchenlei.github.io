<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>基于sasl认证配置libvirt</title>
    <url>/2017/12/15/2017-12-15-%E5%9F%BA%E4%BA%8Esasl%E8%AE%A4%E8%AF%81%E9%85%8D%E7%BD%AElibvirt/</url>
    <content><![CDATA[<h1 id="一、修改libvirtd配置"><a href="#一、修改libvirtd配置" class="headerlink" title="一、修改libvirtd配置"></a>一、修改libvirtd配置</h1><h2 id="1-1-修改-etc-libvirt-libvirtd-conf"><a href="#1-1-修改-etc-libvirt-libvirtd-conf" class="headerlink" title="1.1 修改 /etc/libvirt/libvirtd.conf :"></a>1.1 修改 /etc/libvirt/libvirtd.conf :</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">listen_tls = 0</span><br><span class="line">listen_tcp = 1</span><br><span class="line">tcp_port = "16509"</span><br><span class="line">auth_tcp = "sasl"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">如果启用tls，配置如下</span></span><br><span class="line">auth_tls = "sasl"</span><br></pre></td></tr></table></figure>

<h2 id="1-2-修改-etc-sysconfig-libvirtd"><a href="#1-2-修改-etc-sysconfig-libvirtd" class="headerlink" title="1.2 修改/etc/sysconfig/libvirtd"></a>1.2 修改/etc/sysconfig/libvirtd</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">开启tcp端口</span></span><br><span class="line">LIBVIRTD_ARGS="--listen"</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h1 id="二、添加sasl2配置"><a href="#二、添加sasl2配置" class="headerlink" title="二、添加sasl2配置"></a>二、添加sasl2配置</h1><h2 id="2-1-设置加密方式-etc-sasl2-libvirt-conf"><a href="#2-1-设置加密方式-etc-sasl2-libvirt-conf" class="headerlink" title="2.1 设置加密方式 /etc/sasl2/libvirt.conf"></a>2.1 设置加密方式 /etc/sasl2/libvirt.conf</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mech_list:digest-md5</span><br><span class="line">sasldb_path: /etc/libvirt/passwd.db</span><br></pre></td></tr></table></figure>

<h2 id="2-2-安装所需依赖包："><a href="#2-2-安装所需依赖包：" class="headerlink" title="2.2 安装所需依赖包："></a>2.2 安装所需依赖包：</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">一般我会把所有包装上 yum install cyrus-sasl*</span></span><br><span class="line">yum install cyrus-sasl-md5-2.1.26-21.el7.x86_64 cyrus-sasl-devel-2.1.26-21.el7.x86_64</span><br></pre></td></tr></table></figure>

<h1 id="三、重启libvirtd"><a href="#三、重启libvirtd" class="headerlink" title="三、重启libvirtd"></a>三、重启libvirtd</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">如果调试或者没生效使用：libvirtd --daemon --listen --config /etc/libvirt/libvirtd.conf</span></span><br><span class="line">systemctl restart libvirtd</span><br></pre></td></tr></table></figure>
<h1 id="四、添加用户"><a href="#四、添加用户" class="headerlink" title="四、添加用户"></a>四、添加用户</h1><p>注意这里必须是libvirt，这是application的名字，只有输入libvirt，才会使用配置/etc/sasl2/libvirt.conf，结果才会写到/etc/libvirt/passwd.db</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mercury:~ # saslpasswd2 -a libvirt foxchan                  # add user foxchan</span><br><span class="line">Password: </span><br><span class="line">Again (for verification): </span><br><span class="line"></span><br><span class="line">mercury:~ # sasldblistusers2 -f /etc/libvirt/passwd.db    # list users</span><br><span class="line">foxchan@mercury.example.com: userPassword</span><br><span class="line">mercury:~ # saslpasswd2 -a libvirt -d foxchan                        # delete user foxchan</span><br></pre></td></tr></table></figure>
<p>如果使用qemu+tcp://连接，则应该enable digest-md5，这样交互会被加密。<br>如果使用qemu+tls://连接，则应该disable digest-md5，这样交互就不会被md5加密一次，然后再被TLS加密一次</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> virsh -c qemu+tcp://localhost/system nodeinfo</span></span><br><span class="line">Please enter your authentication name: </span><br><span class="line">Please enter your password: </span><br><span class="line">CPU model:           x86_64</span><br><span class="line">CPU(s):              24</span><br><span class="line">CPU frequency:       1999 MHz</span><br><span class="line">CPU socket(s):       1</span><br><span class="line">Core(s) per socket:  6</span><br><span class="line">Thread(s) per core:  2</span><br><span class="line">NUMA cell(s):        2</span><br><span class="line">Memory size:         134171180 KiB</span><br></pre></td></tr></table></figure>

<p>如果是加密连接，配置如下<br>#对于tcp或者tls则auth</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">auth_tcp = "sasl"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">对于tls需要configure下面的</span></span><br><span class="line"></span><br><span class="line">auth_tls = "sasl"</span><br><span class="line"></span><br><span class="line">key_file = "/etc/pki/libvirt/private/serverkey.pem"</span><br><span class="line"></span><br><span class="line">cert_file = "/etc/pki/libvirt/servercert.pem"</span><br><span class="line"></span><br><span class="line">ca_file = "/etc/pki/CA/cacert.pem"</span><br></pre></td></tr></table></figure>

<p>创建key和certificate</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;Create the Certificate Authority Certificate</span><br><span class="line"></span><br><span class="line"># cat certificate_authority_template.info</span><br><span class="line"></span><br><span class="line">cn &#x3D; libvirt.org</span><br><span class="line">ca</span><br><span class="line">cert_signing_key</span><br><span class="line"></span><br><span class="line">apt-get install gnutls-bin</span><br><span class="line"></span><br><span class="line">(umask 277 &amp;&amp; certtool --generate-privkey &gt; certificate_authority_key.pem)</span><br><span class="line"></span><br><span class="line"># cat certificate_authority_key.pem </span><br><span class="line"></span><br><span class="line">-----BEGIN RSA PRIVATE KEY-----</span><br><span class="line">MIIFfQIBAAKCATEA6VlhWPP0oNt6wVdIXMQZWiyzUc8sNzFkP1K86vgb2z&#x2F;tlX3B</span><br><span class="line">D1bNTCHVo&#x2F;2TeCmHyv4ae3kBxmnB0YST928Yh&#x2F;lbn3q4wayrqIwf&#x2F;MnY9Sm16h7j</span><br><span class="line">P53x&#x2F;1aD6u1IMKxh5TlGIlOXVwlsbFjef&#x2F;Wl2EZGeRlNvm8c0Cwci+8cvsqWXofv</span><br><span class="line">NoN6mxseAxhxYHD1cxsR5gE+h0O5eqpLAZkhaXZOIg7mpbwjSNkErR5Wt5CLAbu7</span><br><span class="line">cqr5PLqxmirTRd7ThIJOQAfaH+m3RzuWbeD4G7DBmNDfm8n5xtkyWEOOmuZqp2+p</span><br><span class="line">wt&#x2F;IYHuH9dH0JRrEPLsX&#x2F;oOhBkiLpRqzAJHZJiqNc9zloYWkGikfHcH7FE9Rj0uc</span><br><span class="line">GRVJfN49DstHhLb&#x2F;0t0eKtZ3WRjfwYI5pA9HUQIDAQABAoIBMBSyITCm&#x2F;mDP5nzE</span><br><span class="line">VX4oxEjboxHT6ouAnXACUhAS7kGNU3CJRTirjay9aXnQcSZcpJcL32RpEI+87Hw9</span><br><span class="line">InR1DXtt0cL8uusuedEKkIG2kz&#x2F;3MZOwpp4LT53CWhPZdKggedQ&#x2F;NqmvuUH2a3&#x2F;W</span><br><span class="line">h&#x2F;FYY5uha&#x2F;mslVkY+Li72NJOZEHFrP14V+6BTbv49Tn4DoIrspFBat536MhNPYhj</span><br><span class="line">FNAS7BMqPCx8t&#x2F;51b8jUuM+TITItuG24bdMsy67w3Xc2Xi3pdS7rc67b31TUMiD7</span><br><span class="line">G0C3uPtNr7uSKVGH6NPDyWZvWgraN91Ls76qIt+felwBBF0uPPIuV8Vr30z0M+MQ</span><br><span class="line">GJNoxtFwwG9q+sQpb38MKbx0e6Vbuig9Mp44i2T6B&#x2F;zFlf1Wt8hyYYXgbno8B7zZ</span><br><span class="line">TjklO7kCgZkA8l86tIPD6+d5E9HhBLeZwxuSdZuXigqDjgqztuvzxhWyESnRykZI</span><br><span class="line">&#x2F;0yZcv&#x2F;haFWpbLYtOcspNxaea34yxwY3saf89M&#x2F;7mQmAJDAE1OwgOZEu0+CQ83fy</span><br><span class="line">pL9byKPH++lUYkZEjlvfTOoLMxDajGLhT6dsW3G6WvAfucX8EoAQaDAHbadyLNLP</span><br><span class="line">J661EQ1s+IDLaFNqTRdawc0CgZkA9nhGBkQPYh6Xq&#x2F;TWuOV1+wrVbZrPrcQnrGm0</span><br><span class="line">HR6i&#x2F;YUKlzr&#x2F;PHjq8BpfcA7oW4R2uvTkKIpc81Z6TFvwIYneMjOT2TFOnFmQQr6F</span><br><span class="line">lfOJHnd2XnEy+xMAb0b3aiSZChbgLE9EnnGuHIg&#x2F;a&#x2F;8Z8pL&#x2F;VnmUO3a6ZEWZHomW</span><br><span class="line">O5nkjCRd6ed86&#x2F;f4C&#x2F;j8sTUyg6NoF3zYkKJPZ5UCgZkAqdDrMts3hKvz+10MCeAS</span><br><span class="line">Dc&#x2F;duCVB8egX8IezDzEW5e0BjGD+qnjAYI&#x2F;s29RIvG94e8DQwzODNyPT74DGVwgW</span><br><span class="line">MZV&#x2F;+I9YuwsbCz&#x2F;RwtWGZo9AfhdTuCKMkb25s0R9tBTxmMGe+xeHxz4chrUCS2Zk</span><br><span class="line">cwjdNNzkMx8XO3RqfxPXawhry&#x2F;qMVreFiUoxdzcu6JZk7j2ztzvrzOkCgZkAxDAy</span><br><span class="line">1TQB5sGhS6CF9wG36w&#x2F;RmgBcBd6ZEfXGCFPlu9XV+1Fb7&#x2F;&#x2F;0SxFDHMjRDmAfvmvG</span><br><span class="line">3bpdiNkyWmvodlnvA5jovD8yauQkH+zWGumTISxIjZ6fzRlwBCPCN7xQEabp5Hto</span><br><span class="line">jJLC3DVb&#x2F;pQ+TEfUdjoq0zaL4dtAqf0tF95E&#x2F;YTJOLIqTw7AZI9aXZiR0BUXe3LG</span><br><span class="line">VELO4A0CgZkAwF78LboWjR9+LGrXu0taaoLymhxltpC81BRWEXjPZ3rj0IEMxITw</span><br><span class="line">9mQAMXdJzXiARoIu&#x2F;sarLb&#x2F;wphVSia2QO8x0Q65De526jUUCZjP&#x2F;wYgl0X8S7Mu+</span><br><span class="line">r3FcApmGyoh+Vy8txyV3uJVh5xbsxF60fSenswQd+VPdJ01XJCbnLbVFH&#x2F;5iQw4a</span><br><span class="line">NzjF&#x2F;UlXfBI0&#x2F;NMSIYP5DnQ&#x3D;</span><br><span class="line">-----END RSA PRIVATE KEY-----</span><br><span class="line"></span><br><span class="line">root@popsuper1982:&#x2F;home&#x2F;cliu8&#x2F;keys&#x2F;certtool# ls -la certificate_authority_key.pem</span><br><span class="line">-r-------- 1 root root 1972 Jul 15 22:28 certificate_authority_key.pem</span><br><span class="line"></span><br><span class="line"># certtool --generate-self-signed --template certificate_authority_template.info --load-privkey certificate_authority_key.pem --outfile certificate_authority_certificate.pem</span><br><span class="line"></span><br><span class="line"># certtool --generate-self-signed --template certificate_authority_template.info --load-privkey certificate_authority_key.pem --outfile certificate_authority_certificate.pem</span><br><span class="line"></span><br><span class="line">Generating a self signed certificate...</span><br><span class="line">X.509 Certificate Information:</span><br><span class="line">        Version: 3</span><br><span class="line">        Serial Number (hex): 53c53b4d</span><br><span class="line">        Validity:</span><br><span class="line">                Not Before: Tue Jul 15 14:31:41 UTC 2014</span><br><span class="line">                Not After: Wed Jul 15 14:31:41 UTC 2015</span><br><span class="line">        Subject: CN&#x3D;libvirt.org</span><br><span class="line">        Subject Public Key Algorithm: RSA</span><br><span class="line">        Certificate Security Level: Normal</span><br><span class="line">                Modulus (bits 2432):</span><br><span class="line">                        00:e9:59:61:58:f3:f4:a0:db:7a:c1:57:48:5c:c4:19</span><br><span class="line">                        5a:2c:b3:51:cf:2c:37:31:64:3f:52:bc:ea:f8:1b:db</span><br><span class="line">                        3f:ed:95:7d:c1:0f:56:cd:4c:21:d5:a3:fd:93:78:29</span><br><span class="line">                        87:ca:fe:1a:7b:79:01:c6:69:c1:d1:84:93:f7:6f:18</span><br><span class="line">                        87:f9:5b:9f:7a:b8:c1:ac:ab:a8:8c:1f:fc:c9:d8:f5</span><br><span class="line">                        29:b5:ea:1e:e3:3f:9d:f1:ff:56:83:ea:ed:48:30:ac</span><br><span class="line">                        61:e5:39:46:22:53:97:57:09:6c:6c:58:de:7f:f5:a5</span><br><span class="line">                        d8:46:46:79:19:4d:be:6f:1c:d0:2c:1c:8b:ef:1c:be</span><br><span class="line">                        ca:96:5e:87:ef:36:83:7a:9b:1b:1e:03:18:71:60:70</span><br><span class="line">                        f5:73:1b:11:e6:01:3e:87:43:b9:7a:aa:4b:01:99:21</span><br><span class="line">                        69:76:4e:22:0e:e6:a5:bc:23:48:d9:04:ad:1e:56:b7</span><br><span class="line">                        90:8b:01:bb:bb:72:aa:f9:3c:ba:b1:9a:2a:d3:45:de</span><br><span class="line">                        d3:84:82:4e:40:07:da:1f:e9:b7:47:3b:96:6d:e0:f8</span><br><span class="line">                        1b:b0:c1:98:d0:df:9b:c9:f9:c6:d9:32:58:43:8e:9a</span><br><span class="line">                        e6:6a:a7:6f:a9:c2:df:c8:60:7b:87:f5:d1:f4:25:1a</span><br><span class="line">                        c4:3c:bb:17:fe:83:a1:06:48:8b:a5:1a:b3:00:91:d9</span><br><span class="line">                        26:2a:8d:73:dc:e5:a1:85:a4:1a:29:1f:1d:c1:fb:14</span><br><span class="line">                        4f:51:8f:4b:9c:19:15:49:7c:de:3d:0e:cb:47:84:b6</span><br><span class="line">                        ff:d2:dd:1e:2a:d6:77:59:18:df:c1:82:39:a4:0f:47</span><br><span class="line">                        51</span><br><span class="line">                Exponent (bits 24):</span><br><span class="line">                        01:00:01</span><br><span class="line">        Extensions:</span><br><span class="line">                Basic Constraints (critical):</span><br><span class="line">                        Certificate Authority (CA): TRUE</span><br><span class="line">                Key Usage (critical):</span><br><span class="line">                        Certificate signing.</span><br><span class="line">                Subject Key Identifier (not critical):</span><br><span class="line">                        f113e5356c2b30271aa1f1bb6183676edee28ed7</span><br><span class="line">Other Information:</span><br><span class="line">        Public Key Id:</span><br><span class="line">                f113e5356c2b30271aa1f1bb6183676edee28ed7</span><br><span class="line"></span><br><span class="line">Signing certificate...</span><br><span class="line"></span><br><span class="line"># cat certificate_authority_certificate.pem </span><br><span class="line"></span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIIDTTCCAgWgAwIBAgIEU8U7TTANBgkqhkiG9w0BAQsFADAWMRQwEgYDVQQDEwts</span><br><span class="line">aWJ2aXJ0Lm9yZzAeFw0xNDA3MTUxNDMxNDFaFw0xNTA3MTUxNDMxNDFaMBYxFDAS</span><br><span class="line">BgNVBAMTC2xpYnZpcnQub3JnMIIBUjANBgkqhkiG9w0BAQEFAAOCAT8AMIIBOgKC</span><br><span class="line">ATEA6VlhWPP0oNt6wVdIXMQZWiyzUc8sNzFkP1K86vgb2z&#x2F;tlX3BD1bNTCHVo&#x2F;2T</span><br><span class="line">eCmHyv4ae3kBxmnB0YST928Yh&#x2F;lbn3q4wayrqIwf&#x2F;MnY9Sm16h7jP53x&#x2F;1aD6u1I</span><br><span class="line">MKxh5TlGIlOXVwlsbFjef&#x2F;Wl2EZGeRlNvm8c0Cwci+8cvsqWXofvNoN6mxseAxhx</span><br><span class="line">YHD1cxsR5gE+h0O5eqpLAZkhaXZOIg7mpbwjSNkErR5Wt5CLAbu7cqr5PLqxmirT</span><br><span class="line">Rd7ThIJOQAfaH+m3RzuWbeD4G7DBmNDfm8n5xtkyWEOOmuZqp2+pwt&#x2F;IYHuH9dH0</span><br><span class="line">JRrEPLsX&#x2F;oOhBkiLpRqzAJHZJiqNc9zloYWkGikfHcH7FE9Rj0ucGRVJfN49DstH</span><br><span class="line">hLb&#x2F;0t0eKtZ3WRjfwYI5pA9HUQIDAQABo0MwQTAPBgNVHRMBAf8EBTADAQH&#x2F;MA8G</span><br><span class="line">A1UdDwEB&#x2F;wQFAwMHBAAwHQYDVR0OBBYEFPET5TVsKzAnGqHxu2GDZ27e4o7XMA0G</span><br><span class="line">CSqGSIb3DQEBCwUAA4IBMQBYPjJmkdduz+ZjoOb&#x2F;4TLrRd7vt+dv88&#x2F;uW1YGO5v8</span><br><span class="line">doYBOGbo&#x2F;KKp0bq&#x2F;&#x2F;TDs8ILl666wu0VeXOSTvvGn6vhVAzttqWmtDgKzvs5E9KGE</span><br><span class="line">N+7O7vPvPooLdKo1hXkXp7S3VrAenHbyqXAff&#x2F;zV8jzDZkeEtaGJ1vlMs4pz3owU</span><br><span class="line">nMX9ndojP4aXdq9B+ny1RyYPGofL7zpjgwocc&#x2F;ubBgu1iRtDN4emHarnjZM6fHkN</span><br><span class="line">HV8QlZgBAoObChXs+zSws&#x2F;nkkS+FPFBGfJSwSjn6AmmGsBHo0KTmN+tkoNZlg6YN</span><br><span class="line">tYHjllz34D5BH5hOmIeEQNSspFdhbE3aK9AqYOHEM99nhDKxiiKhKVgrux3WaiJi</span><br><span class="line">yhanGVlwILFiFtoY&#x2F;6rREIa88jWB1Lxjg88XqlNelqky</span><br><span class="line">-----END CERTIFICATE-----</span><br></pre></td></tr></table></figure>



<p>查看证书</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls -la certificate_authority_certificate.pem</span><br><span class="line">-r-------- 1 root root 1204 Jul 15 22:31 certificate_authority_certificate.pem</span><br><span class="line"></span><br><span class="line">cp certificate_authority_certificate.pem /etc/pki/CA/cacert.pem</span><br><span class="line"></span><br><span class="line">chmod 444 /etc/pki/CA/cacert.pem</span><br><span class="line"></span><br><span class="line">scp -r certificate_authority_certificate.pem cliu8@16.158.166.197:/home/cliu8/</span><br><span class="line"></span><br><span class="line">on 16.158.166.197</span><br><span class="line">cp certificate_authority_certificate.pem /etc/pki/CA/cacert.pem</span><br><span class="line">/////////////////////////Creating Transport Layer Security Server Certificates for libvirt</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat popsuper1982_server_template.info </span></span><br><span class="line">organization = libvirt.org</span><br><span class="line">cn = popsuper1982</span><br><span class="line">tls_www_server</span><br><span class="line">encryption_key</span><br><span class="line">signing_key</span><br><span class="line"></span><br><span class="line">(umask 277 &amp;&amp; certtool --generate-privkey &gt; popsuper1982_server_key.pem)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> (<span class="built_in">umask</span> 277 &amp;&amp; certtool --generate-privkey &gt; popsuper1982_server_key.pem)</span></span><br><span class="line">Generating a 2432 bit RSA private key...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> certtool --generate-certificate --template popsuper1982_server_template.info --load-privkey popsuper1982_server_key.pem --load-ca-certificate certificate_authority_certificate.pem --load-ca-privkey certificate_authority_key.pem --outfile popsuper1982_server_certificate.pem</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> certtool --generate-certificate --template popsuper1982_server_template.info --load-privkey popsuper1982_server_key.pem --load-ca-certificate certificate_authority_certificate.pem --load-ca-privkey certificate_authority_key.pem --outfile popsuper1982_server_certificate.pem</span></span><br><span class="line">Generating a signed certificate...</span><br><span class="line">X.509 Certificate Information:</span><br><span class="line">        Version: 3</span><br><span class="line">        Serial Number (hex): 53c55255</span><br><span class="line">        Validity:</span><br><span class="line">                Not Before: Tue Jul 15 16:09:57 UTC 2014</span><br><span class="line">                Not After: Wed Jul 15 16:09:57 UTC 2015</span><br><span class="line">        Subject: O=libvirt.org,CN=popsuper1982</span><br><span class="line">        Subject Public Key Algorithm: RSA</span><br><span class="line">        Certificate Security Level: Normal</span><br><span class="line">                Modulus (bits 2432):</span><br><span class="line">                        00:ca:e5:92:4f:d8:14:f4:e2:26:26:88:5a:b4:fd:0e</span><br><span class="line">                        ee:86:c6:e8:15:b2:4f:36:6d:8a:b1:23:75:80:b1:0a</span><br><span class="line">                        72:e0:6c:a0:06:ce:03:43:12:2c:6f:e9:ee:bf:21:d2</span><br><span class="line">                        93:82:02:74:14:50:17:fc:f8:21:ec:a6:66:29:9f:d7</span><br><span class="line">                        c3:63:35:50:82:f2:30:9a:12:3b:3e:5a:d1:68:29:7a</span><br><span class="line">                        eb:18:10:7a:69:f8:0b:aa:1a:d9:2d:8b:e4:b3:1b:81</span><br><span class="line">                        c7:de:a8:9f:08:1d:d0:85:db:af:cc:08:00:9b:c1:5a</span><br><span class="line">                        31:f8:9a:43:01:42:88:11:91:35:73:df:57:b9:ce:83</span><br><span class="line">                        34:91:70:ee:29:4b:88:dd:f2:7a:16:1d:c1:36:4d:8b</span><br><span class="line">                        28:7b:05:ba:42:b1:63:8a:92:f8:3d:0c:d1:a2:d8:24</span><br><span class="line">                        92:29:de:c7:8a:73:1d:1e:ec:d3:72:4a:7e:7a:a1:43</span><br><span class="line">                        85:fa:85:ab:58:a3:67:03:4a:48:62:04:b3:f4:8e:f5</span><br><span class="line">                        f0:16:fb:24:28:75:d7:1d:43:52:bc:08:9d:3e:d0:38</span><br><span class="line">                        7d:36:9b:1b:f5:81:0c:17:fd:ba:1b:10:bb:1b:69:cf</span><br><span class="line">                        eb:c1:54:4e:99:ed:ff:15:71:11:9a:63:31:9d:0f:7b</span><br><span class="line">                        b2:31:a2:4c:49:2a:ee:d1:3c:80:c0:98:e0:6c:63:6b</span><br><span class="line">                        52:9a:b5:d6:0b:6c:34:13:86:d6:ab:c4:4d:9a:ff:ae</span><br><span class="line">                        be:60:b2:a7:e3:3d:b4:63:05:d3:f0:f9:69:01:35:97</span><br><span class="line">                        1c:52:48:24:75:b5:73:96:b5:fb:bc:8e:2e:62:7c:5c</span><br><span class="line">                        17</span><br><span class="line">                Exponent (bits 24):</span><br><span class="line">                        01:00:01</span><br><span class="line">        Extensions:</span><br><span class="line">                Basic Constraints (critical):</span><br><span class="line">                        Certificate Authority (CA): FALSE</span><br><span class="line">                Key Purpose (not critical):</span><br><span class="line">                        TLS WWW Server.</span><br><span class="line">                Key Usage (critical):</span><br><span class="line">                        Digital signature.</span><br><span class="line">                        Key encipherment.</span><br><span class="line">                Subject Key Identifier (not critical):</span><br><span class="line">                        0570b6cf340118f7d44dd67c0ca6ffbbb7a4871c</span><br><span class="line">                Authority Key Identifier (not critical):</span><br><span class="line">                        f113e5356c2b30271aa1f1bb6183676edee28ed7</span><br><span class="line">Other Information:</span><br><span class="line">        Public Key Id:</span><br><span class="line">                0570b6cf340118f7d44dd67c0ca6ffbbb7a4871c</span><br><span class="line"></span><br><span class="line">Signing certificate...</span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cp popsuper1982_server_certificate.pem /etc/pki/libvirt/servercert.pem</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp popsuper1982_server_key.pem /etc/pki/libvirt/private/serverkey.pem</span></span><br><span class="line"></span><br><span class="line">///////////////////////Creating Transport Layer Security Client Certificates for libvirt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat escto-bj-hp-z620_client_template.info </span></span><br><span class="line">country = CN</span><br><span class="line">state = Beijing</span><br><span class="line">locality = Beijing</span><br><span class="line">organization = libvirt.org</span><br><span class="line">cn = escto-bj-hp-z620</span><br><span class="line">tls_www_client</span><br><span class="line">encryption_key</span><br><span class="line">signing_key</span><br><span class="line">(umask 277 &amp;&amp; certtool --generate-privkey &gt; escto-bj-hp-z620_client_key.pem)</span><br><span class="line"></span><br><span class="line"> (umask 277 &amp;&amp; certtool --generate-privkey &gt; escto-bj-hp-z620_client_key.pem)</span><br><span class="line">Generating a 2432 bit RSA private key...</span><br><span class="line"></span><br><span class="line"> certtool --generate-certificate --template escto-bj-hp-z620_client_template.info --load-privkey escto-bj-hp-z620_client_key.pem --load-ca-certificate certificate_authority_certificate.pem --load-ca-privkey certificate_authority_key.pem --outfile escto-bj-hp-z620_client_certificate.pem</span><br><span class="line"></span><br><span class="line"> certtool --generate-certificate --template escto-bj-hp-z620_client_template.info --load-privkey escto-bj-hp-z620_client_key.pem --load-ca-certificate certificate_authority_certificate.pem --load-ca-privkey certificate_authority_key.pem --outfile escto-bj-hp-z620_client_certificate.pem</span><br><span class="line">Generating a signed certificate...</span><br><span class="line">X.509 Certificate Information:</span><br><span class="line">        Version: 3</span><br><span class="line">        Serial Number (hex): 53c55678</span><br><span class="line">        Validity:</span><br><span class="line">                Not Before: Tue Jul 15 16:27:36 UTC 2014</span><br><span class="line">                Not After: Wed Jul 15 16:27:36 UTC 2015</span><br><span class="line">        Subject: C=CN,O=libvirt.org,L=Beijing,ST=Beijing,CN=escto-bj-hp-z620</span><br><span class="line">        Subject Public Key Algorithm: RSA</span><br><span class="line">        Certificate Security Level: Normal</span><br><span class="line">                Modulus (bits 2432):</span><br><span class="line">                        00:9f:5c:d7:89:2c:0b:fe:e7:38:21:1d:2a:da:92:eb</span><br><span class="line">                        82:91:96:3c:f3:22:28:a3:d6:3d:70:68:27:a1:2b:3a</span><br><span class="line">                        3e:79:03:55:ea:1b:eb:1f:81:90:71:3d:41:4c:99:f2</span><br><span class="line">                        96:46:22:e0:33:91:bb:5b:38:83:a5:3f:d7:0a:25:dd</span><br><span class="line">                        f0:90:cf:0d:4a:18:b3:0d:db:01:5f:70:78:1b:63:01</span><br><span class="line">                        ac:6f:32:8b:79:cc:51:0f:d4:45:1e:6f:b8:d8:84:57</span><br><span class="line">                        af:ca:e8:3e:58:0a:79:89:0d:99:e2:8a:b2:2c:85:dc</span><br><span class="line">                        40:15:7c:52:9b:46:66:a0:0f:6c:52:7d:e8:1b:b4:06</span><br><span class="line">                        5a:d9:7f:30:84:d0:37:66:2a:3f:53:50:33:99:93:b7</span><br><span class="line">                        8d:a2:74:cd:4d:ca:76:b5:df:2f:47:f9:e2:1a:7b:91</span><br><span class="line">                        49:20:be:c9:ca:9b:f1:2c:f1:40:1a:66:37:2e:ac:23</span><br><span class="line">                        3f:3d:44:de:0f:2d:d2:60:63:22:2d:49:28:98:c0:b7</span><br><span class="line">                        9f:70:be:51:b8:d3:10:1d:40:3d:3e:6f:1a:f1:a2:1b</span><br><span class="line">                        5c:2e:68:8b:05:99:7b:0b:5c:9c:78:54:8f:de:ae:7e</span><br><span class="line">                        75:6d:fb:af:cc:ea:3c:be:9d:25:57:d1:3d:ef:27:66</span><br><span class="line">                        c3:93:2f:62:f0:43:31:64:df:df:20:b6:b8:df:8a:85</span><br><span class="line">                        77:38:9b:3d:85:e1:67:1b:e6:b4:9b:e0:30:4e:6c:62</span><br><span class="line">                        c1:07:72:57:ad:bc:fc:f0:0c:d8:d4:12:ac:eb:80:ee</span><br><span class="line">                        04:b2:b0:10:e4:d6:71:68:2b:ee:e6:98:23:9a:df:c7</span><br><span class="line">                        95</span><br><span class="line">                Exponent (bits 24):</span><br><span class="line">                        01:00:01</span><br><span class="line">        Extensions:</span><br><span class="line">                Basic Constraints (critical):</span><br><span class="line">                        Certificate Authority (CA): FALSE</span><br><span class="line">                Key Purpose (not critical):</span><br><span class="line">                        TLS WWW Client.</span><br><span class="line">                Key Usage (critical):</span><br><span class="line">                        Digital signature.</span><br><span class="line">                        Key encipherment.</span><br><span class="line">                Subject Key Identifier (not critical):</span><br><span class="line">                        f4385711f8089b39ba2c13869aa816114d378190</span><br><span class="line">                Authority Key Identifier (not critical):</span><br><span class="line">                        f113e5356c2b30271aa1f1bb6183676edee28ed7</span><br><span class="line">Other Information:</span><br><span class="line">        Public Key Id:</span><br><span class="line">                f4385711f8089b39ba2c13869aa816114d378190</span><br><span class="line"></span><br><span class="line">Signing certificate...</span><br><span class="line"></span><br><span class="line">root@popsuper1982:/home/cliu8/keys/certtool# scp escto-bj-hp-z620_client_certificate.pem cliu8@16.158.166.197:/home/cliu8/</span><br><span class="line">cliu8@16.158.166.197's password: </span><br><span class="line">escto-bj-hp-z620_client_certificate.pem                                                    100% 1379     1.4KB/s   00:00    </span><br><span class="line">root@popsuper1982:/home/cliu8/keys/certtool# scp escto-bj-hp-z620_client_key.pem cliu8@16.158.166.197:/home/cliu8/        </span><br><span class="line">cliu8@16.158.166.197's password: </span><br><span class="line">escto-bj-hp-z620_client_key.pem                        100% 1968     1.9KB/s   00:00</span><br></pre></td></tr></table></figure>
<p>​                                    </p>
<p>修改/etc/hosts文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">16.158.166.150  popsuper1982</span><br><span class="line">16.158.166.197  escto-bj-hp-z620</span><br></pre></td></tr></table></figure>



<p>修改/etc/default/libvirt-bin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">cat /etc/default/libvirt-bin</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Defaults <span class="keyword">for</span> libvirt-bin initscript (/etc/init.d/libvirt-bin)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> This is a POSIX shell fragment</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Start libvirtd to handle qemu/kvm:</span></span><br><span class="line"></span><br><span class="line">start_libvirtd="yes"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> options passed to libvirtd, add <span class="string">"-l"</span> to listen on tcp</span></span><br><span class="line"></span><br><span class="line">libvirtd_opts="-d -l"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> pass <span class="keyword">in</span> location of kerberos keytab</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> KRB5_KTNAME=/etc/libvirt/libvirt.keytab</span></span><br></pre></td></tr></table></figure>



<p>重启libvirt-bin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@popsuper1982:/home/cliu8# netstat -na | grep 16509</span><br><span class="line">tcp        0      0 0.0.0.0:16509           0.0.0.0:*               LISTEN     </span><br><span class="line">tcp6       0      0 :::16509                :::*                    LISTEN     </span><br><span class="line">root@popsuper1982:/home/cliu8# netstat -na | grep 16514</span><br><span class="line">tcp        0      0 0.0.0.0:16514           0.0.0.0:*               LISTEN     </span><br><span class="line">tcp6       0      0 :::16514                :::*                    LISTEN</span><br></pre></td></tr></table></figure>

<p>如果远程连接tls，则需要配置/etc/pki/CA/cacert.pem</p>
<p>在client机器上escto-bj-hp-z620</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> tree --charset ASCII /etc/pki/</span><br><span class="line">/etc/pki/</span><br><span class="line">|-- CA</span><br><span class="line">|   `-- cacert.pem</span><br><span class="line">|-- libvirt</span><br><span class="line">|   |-- clientcert.pem</span><br><span class="line">|   `-- private</span><br><span class="line">|       `-- clientkey.pem</span><br><span class="line">`-- nssdb -&gt; /var/lib/nssdb</span><br></pre></td></tr></table></figure>



<p>在server机器上popsuper1982</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># tree --charset ASCII &#x2F;etc&#x2F;pki&#x2F;</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;pki&#x2F;</span><br><span class="line">|-- CA</span><br><span class="line">|   &#96;-- cacert.pem</span><br><span class="line">|-- libvirt</span><br><span class="line">|   |-- private</span><br><span class="line">|   |   &#96;-- serverkey.pem</span><br><span class="line">|   &#96;-- servercert.pem</span><br><span class="line">&#96;-- nssdb -&gt; &#x2F;var&#x2F;lib&#x2F;nssdb</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> virsh -c qemu+tls://popsuper1982/system list --all</span></span><br><span class="line"></span><br><span class="line">Please enter your authentication name: test1@popsuper1982</span><br><span class="line">Please enter your password: </span><br><span class="line"></span><br><span class="line">Id    Name                           State</span><br><span class="line">----------------------------------------------------</span><br><span class="line"></span><br><span class="line">-     ubuntu-14.04                   shut off</span><br></pre></td></tr></table></figure>



<p>需要用hostname才能通过认证</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>libvrit</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx动态限流</title>
    <url>/2018/07/10/2018-07-10-nginx-%E5%8A%A8%E6%80%81%E9%99%90%E6%B5%81/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>动态限流一般是lua+redis,需要添加lua模块，或者把nginx直接替换为openresty.这个涉及到自己实现lua脚本，对于小白是有些难度的，这里利用第三方模块去实现，比lua要简单些，快速实现动态限流。</p>
<p>以下是nginx官方的第三方模块列表<br><a href="https://www.nginx.com/resources/wiki/modules/index.html" target="_blank" rel="noopener" title="NGINX 3rd Party Modules">NGINX 3rd Party Modules</a></p>
<p>我们使用<a href="https://github.com/limithit/ngx_dynamic_limit_req_module" target="_blank" rel="noopener">ngx_dynamic_limit_req_module</a>，配合redis实现动态限流</p>
<a id="more"></a>

<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h2><p>hiredis ： redis官方 C api<br>nginx   :  版本必须高于1.9.11.</p>
<h2 id="下载并编译hiredis"><a href="#下载并编译hiredis" class="headerlink" title="下载并编译hiredis"></a>下载并编译hiredis</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root hiredis]# wget https:&#x2F;&#x2F;github.com&#x2F;redis&#x2F;hiredis&#x2F;archive&#x2F;v0.13.3.tar.gz</span><br><span class="line">[root hiredis]# tar -zxf v0.13.3.tar.gz</span><br><span class="line">[root hiredis]# cd hiredis-0.13.3&#x2F;</span><br><span class="line">[root hiredis]# make</span><br><span class="line">[root hiredis]# make install</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;local&#x2F;include&#x2F;hiredis &#x2F;usr&#x2F;local&#x2F;lib</span><br><span class="line">cp -a hiredis.h async.h read.h sds.h adapters &#x2F;usr&#x2F;local&#x2F;include&#x2F;hiredis</span><br><span class="line">cp -a libhiredis.so &#x2F;usr&#x2F;local&#x2F;lib&#x2F;libhiredis.so.0.13</span><br><span class="line">cd &#x2F;usr&#x2F;local&#x2F;lib &amp;&amp; ln -sf libhiredis.so.0.13 libhiredis.so</span><br><span class="line">cp -a libhiredis.a &#x2F;usr&#x2F;local&#x2F;lib</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;local&#x2F;lib&#x2F;pkgconfig</span><br><span class="line">cp -a hiredis.pc &#x2F;usr&#x2F;local&#x2F;lib&#x2F;pkgconfig</span><br></pre></td></tr></table></figure>

<h2 id="下载并安装nginx"><a href="#下载并安装nginx" class="headerlink" title="下载并安装nginx"></a>下载并安装nginx</h2><p>因为原生nginx没有upstream检查，需要添加第三方模块</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;yaoweibin&#x2F;nginx_upstream_check_module&#x2F;archive&#x2F;master.zip</span><br><span class="line">unzip master.zip</span><br></pre></td></tr></table></figure>
<p>开始安装nginx</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.14.0.tar.gz</span><br><span class="line">tar -zxf nginx-1.14.0.tar.gz</span><br><span class="line">cd nginx-1.14.0&#x2F;</span><br><span class="line"> .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --with-pcre&#x3D;&#x2F;usr&#x2F;local&#x2F;src&#x2F;pcre-8.42 --with-http_stub_status_module  --with-http_ssl_module --with-http_v2_module  --add-module&#x3D;&#x2F;usr&#x2F;local&#x2F;src&#x2F;ngx_dynamic_limit_req_module-1.7 --add-module&#x3D;&#x2F;usr&#x2F;local&#x2F;src&#x2F;nginx_upstream_check_module-master</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>检查hiredis是否安装成功，如果是not found,就建个软链</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ldd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx|grep libhiredis</span><br><span class="line">libhiredis.so.0.13 &#x3D;&gt; not found (0x00007fd0fa245000)</span><br><span class="line">#查看之前生成的 so文件路径并软链</span><br><span class="line">&#x2F;lib&#x2F;libhiredis.so.0.13 -&gt; &#x2F;usr&#x2F;local&#x2F;lib&#x2F;libhiredis.so.0.13</span><br></pre></td></tr></table></figure>

<h2 id="这时候就可以装个redis了"><a href="#这时候就可以装个redis了" class="headerlink" title="这时候就可以装个redis了"></a>这时候就可以装个redis了</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install redis -y</span><br><span class="line">systemctl start redis</span><br></pre></td></tr></table></figure>

<h1 id="配置文件模板"><a href="#配置文件模板" class="headerlink" title="配置文件模板"></a>配置文件模板</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">worker_processes  2;</span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application&#x2F;octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line">    </span><br><span class="line">    dynamic_limit_req_zone $binary_remote_addr zone&#x3D;one:10m rate&#x3D;100r&#x2F;s redis&#x3D;127.0.0.1 block_second&#x3D;300;</span><br><span class="line">    dynamic_limit_req_zone $binary_remote_addr zone&#x3D;two:10m rate&#x3D;50r&#x2F;s redis&#x3D;127.0.0.1 block_second&#x3D;600;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost;</span><br><span class="line">        location &#x2F; &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">            dynamic_limit_req zone&#x3D;one burst&#x3D;5 nodelay;</span><br><span class="line">            dynamic_limit_req_status 403;</span><br><span class="line">        &#125;</span><br><span class="line">        error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">        location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost2;</span><br><span class="line">        location &#x2F; &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">            dynamic_limit_req zone&#x3D;two burst&#x3D;5 nodelay;</span><br><span class="line">            dynamic_limit_req_status 403;</span><br><span class="line">        &#125;</span><br><span class="line">        error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">        location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="支持黑白名单"><a href="#支持黑白名单" class="headerlink" title="支持黑白名单"></a>支持黑白名单</h2><h3 id="White-list-rules"><a href="#White-list-rules" class="headerlink" title="White list rules"></a>White list rules</h3><p>redis-cli set whiteip ip<br>举例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis-cli set white192.168.1.1 192.168.1.1</span><br></pre></td></tr></table></figure>
<h3 id="Black-list-rules"><a href="#Black-list-rules" class="headerlink" title="Black list rules"></a>Black list rules</h3><p>redis-cli set ip ip<br>举例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis-cli set 192.168.1.2 192.168.1.2</span><br></pre></td></tr></table></figure>

<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>可以对具体的接口或者页面进行限流，而不用全局限制，block_second是锁定时间，单位为秒，这个时间可以自定义<br>参数也可以放到location里的if</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F; &#123;</span><br><span class="line">         root   html;</span><br><span class="line">         index  index.html index.htm;</span><br><span class="line">          </span><br><span class="line">       if ($document_uri ~* &quot;index.php&quot;)&#123;          </span><br><span class="line">         dynamic_limit_req zone&#x3D;two burst&#x3D;5 nodelay;</span><br><span class="line">         dynamic_limit_req_status 403;</span><br><span class="line">              &#125;</span><br><span class="line">       if ( $args ~ l&#x3D;(.*)$) &#123;</span><br><span class="line">             dynamic_limit_req zone&#x3D;one burst&#x3D;5 nodelay;</span><br><span class="line">             dynamic_limit_req_status 403; </span><br><span class="line">             &#125;</span><br><span class="line">         </span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure>

<h1 id="小知识"><a href="#小知识" class="headerlink" title="小知识"></a>小知识</h1><p>提取apache ab命令单独使用，进行压测</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;tmp</span><br><span class="line">yumdownloader httpd-tools</span><br><span class="line">rpm2cpio httpd-tools-2.4.6-80.el7.centos.1.x86_64.rpm| cpio -idmv </span><br><span class="line">cd &#x2F;tmp&#x2F;usr&#x2F;bin&#x2F;</span><br><span class="line">cp ab &#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>jenkins与gitlab整合</title>
    <url>/2018/07/09/2018-07-09-jenkins-gitlab-webhook/</url>
    <content><![CDATA[<h1 id="1、环境说明"><a href="#1、环境说明" class="headerlink" title="1、环境说明"></a>1、环境说明</h1><p>jenkins: 2.68</p>
<p>gitlab : 10.1.1</p>
<p>tomcat ：6.0.18</p>
<p><strong>注意：不同版本gitlab，配置webhook方式有差异，这里只展示当前版本</strong></p>
<a id="more"></a>

<h1 id="2、相关依赖包"><a href="#2、相关依赖包" class="headerlink" title="2、相关依赖包"></a>2、相关依赖包</h1><p>该功能需要jenkins 安装插件Gitlab Hook Plugin</p>
<h2 id="2-1-jenkins在安装gitlab-hook-plugins提示失败"><a href="#2-1-jenkins在安装gitlab-hook-plugins提示失败" class="headerlink" title="2.1 jenkins在安装gitlab hook plugins提示失败:"></a>2.1 jenkins在安装gitlab hook plugins提示失败:</h2><p>问题 : gitlab hook plugin无法安装的原因是因为ruby-runtime无法安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Failed to dynamically deploy this plugin</span><br><span class="line">at hudson.model.UpdateCenter$InstallationJob._run(UpdateCenter.java:1328)</span><br><span class="line">at hudson.model.UpdateCenter$DownloadJob.run(UpdateCenter.java:1126)</span><br><span class="line">at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">at hudson.remoting.AtmostOneThreadExecutor$Worker.run(AtmostOneThreadExecutor.java:110)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">Caused by: java.io.IOException: Failed to install ruby-runtime plugin</span><br><span class="line">at hudson.PluginManager.dynamicLoad(PluginManager.java:487)</span><br><span class="line">at hudson.model.UpdateCenter$InstallationJob._run(UpdateCenter.java:1324)</span><br><span class="line">... 5 more</span><br><span class="line">Caused by: java.io.IOException: Failed to initialize</span><br><span class="line">at hudson.ClassicPluginStrategy.load(ClassicPluginStrategy.java:441)</span><br><span class="line">at hudson.PluginManager.dynamicLoad(PluginManager.java:478)</span><br><span class="line">... 6 more</span><br><span class="line">Caused by: java.lang.ClassCircularityError: org&#x2F;jruby&#x2F;RubyClass</span><br><span class="line">at java.lang.Class.forName0(Native Method)</span><br><span class="line">at java.lang.Class.forName(Class.java:348)</span><br></pre></td></tr></table></figure>

<p>这是官方issue <a href="https://issues.jenkins-ci.org/browse/JENKINS-31019" target="_blank" rel="noopener">https://issues.jenkins-ci.org/browse/JENKINS-31019</a></p>
<p>处理方式：给jenkins的启动容器添加JAVA_OPTIONS，我这里是tomcat，所以添加JAVA_OPTS到catalina.sh</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JAVA_OPTS&#x3D;&quot;-Djava.awt.headless&#x3D;true -Dhudson.ClassicPluginStrategy.noBytecodeTransformer&#x3D;true&quot;</span><br></pre></td></tr></table></figure>

<h1 id="3、配置gitlab认证"><a href="#3、配置gitlab认证" class="headerlink" title="3、配置gitlab认证"></a>3、配置gitlab认证</h1><img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/1.png" class="">

<p>添加一个新的凭证</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/2.png" class="">

<p>从gitlab的设置中将 token复制过来</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/3.png" class="">

<p>将复制的token粘贴到api token中，点ok</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/4.png" class="">

<p>在系统配置中找到Gitlab 将信息进行填写，Credentials 选择刚刚创建对的即可</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/6.png" class="">
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/5.png" class="">


<p>打开项目，编辑项目的构建触发器</p>
<p>配置触发条件，复制jenkins URL</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/7.png" class="">

<p>生成Secret Token</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/8.png" class="">


<h1 id="4、获取jenkins-用户token"><a href="#4、获取jenkins-用户token" class="headerlink" title="4、获取jenkins 用户token"></a>4、获取jenkins 用户token</h1><p>注意：jenkins用户的token只能是用户自己查看，就算是admin 也看不了</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/9.png" class="">


<h1 id="5、配置gitlab"><a href="#5、配置gitlab" class="headerlink" title="5、配置gitlab"></a>5、配置gitlab</h1><p>URL 格式 ：<a href="http://username:api">http://username:api</a> token@jenkins调用url</p>
<p>secret token 就是之前jenkins project里面生成的</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/10.png" class="">

<p>保存后 ，测试</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/13.png" class="">


<p>返回200 表明成功，而且jenkins已经自动跑了</p>
<img src="/2018/07/09/2018-07-09-jenkins-gitlab-webhook/12.png" class="">







]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>配置shadowsocks服务端</title>
    <url>/2018/08/09/2018-08-09-%E9%85%8D%E7%BD%AEshadowscoks%E6%9C%8D%E5%8A%A1%E7%AB%AF/</url>
    <content><![CDATA[<h1 id="安装shadowsocks"><a href="#安装shadowsocks" class="headerlink" title="安装shadowsocks"></a>安装shadowsocks</h1><p>以centos7 举例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install shadowsocks</span><br></pre></td></tr></table></figure>

<hr>
<a id="more"></a>

<h1 id="配置shadowsocks"><a href="#配置shadowsocks" class="headerlink" title="配置shadowsocks"></a>配置shadowsocks</h1><p>shadowsocks有两种配置方法，一种是命令行参数配置，一种是配置文件。先来说说命令行配置</p>
<h2 id="命令行参数配置"><a href="#命令行参数配置" class="headerlink" title="命令行参数配置"></a>命令行参数配置</h2><p>命令行参数如下：<br>|参数名 | 参数意义|<br>|:–|:–|<br>|-s | 服务器地址<br>|-p | 服务器端口号<br>|-k    |服务器密码<br>|-m    |服务器加密方式<br>|-t    |服务器超时时间<br>|-c    |配置文件路径<br>|–fast-open    |快速打开模式，仅Unix/Linux系统可用<br>|–workers    |工作者数量</p>
<p>每次运行shadowsocks都将一大堆参数传进去是件很麻烦的事情。所以一般情况下都是采用配置文件的方式来配置的。然后通过-c参数将配置文件路径传入。</p>
<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><p>shadowsocks的配置文件是一个json形式的文件，各参数的意义和命令行参数意义相同。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;server&quot;:&quot;my_server_ip&quot;,</span><br><span class="line">    &quot;server_port&quot;:8388,</span><br><span class="line">    &quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;local_port&quot;:1080,</span><br><span class="line">    &quot;password&quot;:&quot;mypassword&quot;,</span><br><span class="line">    &quot;timeout&quot;:300,</span><br><span class="line">    &quot;method&quot;:&quot;aes-256-cfb&quot;,</span><br><span class="line">    &quot;fast_open&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务器地址就写服务器的ip地址，不要写127.0.0.1。端口号可以自己写，但是不要占用其他服务的端口。本地地址和本地端口是客户端使用的，服务端可以不用理会。密码尽量采用复杂一点的密码，以保证安全性。加密方式使用aes-256-cfb就可以了。如果服务器是Linux系统的话，打开fast_open</p>
<p>配置文件编辑完毕之后，就可以运行shadowsocks了。前台运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssserver -c &#x2F;etc&#x2F;shadowsocks&#x2F;config.json</span><br></pre></td></tr></table></figure>
<p>后台运行和停止：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssserver -c &#x2F;etc&#x2F;shadowsocks.json -d start</span><br><span class="line">ssserver -c &#x2F;etc&#x2F;shadowsocks.json -d stop</span><br></pre></td></tr></table></figure>
<p>以上都是在root用户下运行的。以root方式运行可能会有一些安全问题。所以一般都是使用普通用户运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssserver -c &#x2F;etc&#x2F;shadowsocks.json --user nobody -d start</span><br></pre></td></tr></table></figure>

<p>以上都需要自己手动运行命令来启动shadowsocks。要让shadowsocks在系统启动时自动运行，需要在/etc/rc.local中添加命令。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ssserver -c &#x2F;etc&#x2F;shadowsocks.json --user nobody -d start</span><br></pre></td></tr></table></figure>

<h1 id="shadowsocks优化"><a href="#shadowsocks优化" class="headerlink" title="shadowsocks优化"></a>shadowsocks优化</h1><h2 id="提高最大连接数"><a href="#提高最大连接数" class="headerlink" title="提高最大连接数"></a>提高最大连接数</h2><p>编辑/etc/security/limits.conf文件，添加以下两行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* soft nofile 51200</span><br><span class="line">* hard nofile 51200</span><br></pre></td></tr></table></figure>
<p>然后，在启动shadowsocks服务器之前，先设置一下ulimit：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ulimit -n 51200</span><br></pre></td></tr></table></figure>
<h2 id="调整内核参数"><a href="#调整内核参数" class="headerlink" title="调整内核参数"></a>调整内核参数</h2><p>调整内核参数的目标是：</p>
<ul>
<li>尽可能重用连接和端口号</li>
<li>尽可能增大队列和缓冲区</li>
<li>为高延迟和高流量选择合适的TCP拥塞算法</li>
</ul>
<p>下面是一个生产服务器的配置(/etc/sysctl.conf)，实例。如果不能使用，可能需要自己编译内核模块</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fs.file-max &#x3D; 51200</span><br><span class="line"></span><br><span class="line">net.core.rmem_max &#x3D; 67108864</span><br><span class="line">net.core.wmem_max &#x3D; 67108864</span><br><span class="line">net.core.netdev_max_backlog &#x3D; 250000</span><br><span class="line">net.core.somaxconn &#x3D; 4096</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_syncookies &#x3D; 1</span><br><span class="line">net.ipv4.tcp_tw_reuse &#x3D; 1</span><br><span class="line">net.ipv4.tcp_tw_recycle &#x3D; 0</span><br><span class="line">net.ipv4.tcp_fin_timeout &#x3D; 30</span><br><span class="line">net.ipv4.tcp_keepalive_time &#x3D; 1200</span><br><span class="line">net.ipv4.ip_local_port_range &#x3D; 10000 65000</span><br><span class="line">net.ipv4.tcp_max_syn_backlog &#x3D; 8192</span><br><span class="line">net.ipv4.tcp_max_tw_buckets &#x3D; 5000</span><br><span class="line">net.ipv4.tcp_fastopen &#x3D; 3</span><br><span class="line">net.ipv4.tcp_mem &#x3D; 25600 51200 102400</span><br><span class="line">net.ipv4.tcp_rmem &#x3D; 4096 87380 67108864</span><br><span class="line">net.ipv4.tcp_wmem &#x3D; 4096 65536 67108864</span><br><span class="line">net.ipv4.tcp_mtu_probing &#x3D; 1</span><br><span class="line">net.ipv4.tcp_congestion_control &#x3D; hybla</span><br></pre></td></tr></table></figure>
<p>修改之后需要运行sysctl -p来重载配置</p>
<h3 id="转载于"><a href="#转载于" class="headerlink" title="转载于###"></a>转载于###</h3><p><a href="https://blog.csdn.net/u011054333/article/details/52496303" target="_blank" rel="noopener">https://blog.csdn.net/u011054333/article/details/52496303</a></p>
<h1 id="docker-启动"><a href="#docker-启动" class="headerlink" title="docker 启动#"></a>docker 启动#</h1><p>有2个image可以选择<br>简单实用版</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d -p 1984:1984 oddrationale&#x2F;docker-shadowsocks -s 0.0.0.0 -p 1984 -k $SSPASSWORD -m aes-256-cfb</span><br></pre></td></tr></table></figure>

<p>持续更新，附加功能版(集成 kcptun、simple-obfs),具体参数可以查看hub.docker.com</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -dt --name ss -p 6443:6443 mritd&#x2F;shadowsocks -s &quot;-s 0.0.0.0 -p 6443 -m chacha20 -k test123 --fast-open&quot;</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>GreateWall</tag>
      </tags>
  </entry>
  <entry>
    <title>kubeadm安装1.13</title>
    <url>/2018/12/27/2018-12-27-kubeadm%E5%AE%89%E8%A3%851.13/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>kubeadm 快速安装kubernetes集群，kubeadm 主要功能已经GA，除了高可用 还在alpha。功能如下图</p>
<table>
<thead>
<tr>
<th>Area</th>
<th>Maturity Level</th>
</tr>
</thead>
<tbody><tr>
<td>Command line UX</td>
<td>GA</td>
</tr>
<tr>
<td>Implementation</td>
<td>GA</td>
</tr>
<tr>
<td>Config file API</td>
<td>beta</td>
</tr>
<tr>
<td>CoreDNS</td>
<td>GA</td>
</tr>
<tr>
<td>kubeadm alpha subcommands</td>
<td>alpha</td>
</tr>
<tr>
<td>High availability</td>
<td>alpha</td>
</tr>
<tr>
<td>DynamicKubeletConfig</td>
<td>alpha</td>
</tr>
<tr>
<td>Self-hosting</td>
<td>alpha</td>
</tr>
</tbody></table>
<p>当前我们线上稳定运行的Kubernetes集群是使用pod形式部署的高可用集群，这里体验Kubernetes 1.13中的kubeadm是为了了解官方对集群初始化和配置方面的最佳方式</p>
<a id="more"></a>

<h1 id="1-准备"><a href="#1-准备" class="headerlink" title="1.准备"></a>1.准备</h1><h2 id="1-1系统配置"><a href="#1-1系统配置" class="headerlink" title="1.1系统配置"></a>1.1系统配置</h2><p>在安装之前，需要先做如下准备。三台CentOS 7.5主机如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat /etc/hosts</span><br><span class="line">192.168.1.57 master</span><br><span class="line">192.168.1.33 node1</span><br><span class="line">192.168.1.34 node2</span><br></pre></td></tr></table></figure>
<p>如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口，可以查看Installing kubeadm中的”Check required ports”一节。 这里简单起见在各节点禁用防火墙：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>
<p>禁用SELINUX：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure>
<p>创建/etc/sysctl.d/k8s.conf文件，添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure>
<p>如果准备使用ipvs 模式node节点添加keepalive配置如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">net.ipv4.tcp_keepalive_time = 600</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 30</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 10</span><br></pre></td></tr></table></figure>
<p>相关issue：<br><a href="https://github.com/moby/moby/issues/31208" target="_blank" rel="noopener">https://github.com/moby/moby/issues/31208</a></p>
<p>执行命令使修改生效。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">modprobe br_netfilter</span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure>
<p>##1.2kube-proxy开启ipvs的前置条件<br>由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ip_vs</span><br><span class="line">ip_vs_rr</span><br><span class="line">ip_vs_wrr</span><br><span class="line">ip_vs_sh</span><br><span class="line">nf_conntrack_ipv4</span><br></pre></td></tr></table></figure>
<p>在所有的Kubernetes节点node1和node2上执行以下脚本:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br></pre></td></tr></table></figure>
<p>上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。</p>
<p>接下来还需要确保各个节点上已经安装了ipset软件包yum install ipset。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm yum install ipvsadm。</p>
<p>如果以上前提条件如果不满足，则即使kube-proxy的配置开启了ipvs模式，也会退回到iptables模式。</p>
<h2 id="1-3安装Docker"><a href="#1-3安装Docker" class="headerlink" title="1.3安装Docker"></a>1.3安装Docker</h2><p>Kubernetes从1.6开始使用CRI(Container Runtime Interface)容器运行时接口。默认的容器运行时仍然是Docker，使用的是kubelet中内置dockershim CRI实现。</p>
<p>安装docker的yum源:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /etc/yum.repos.d/</span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>
<p>查看最新的Docker版本：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum list docker-ce.x86_64  --showduplicates |sort -r</span><br><span class="line">docker-ce.x86_64            3:18.09.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.1.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.0.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.03.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.03.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.12.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.12.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.09.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.09.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.06.2.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.06.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.06.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.3.ce-1.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.2.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.0.ce-1.el7.centos             docker-ce-stable</span><br></pre></td></tr></table></figure>
<p>Kubernetes 1.12已经针对Docker的1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06等版本做了验证，需要注意Kubernetes 1.12最低支持的Docker版本是1.11.1。Kubernetes 1.13对Docker的版本依赖方面没有变化。</p>
<p>确认一下iptables filter表中FOWARD链的默认策略(pllicy)为ACCEPT。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -nvL</span><br><span class="line">Chain INPUT (policy ACCEPT 263 packets, 19209 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line"></span><br><span class="line">Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0</span><br><span class="line">    0     0 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0</span><br><span class="line">    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED</span><br><span class="line">    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0</span><br><span class="line">    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0</span><br><span class="line">    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0</span><br></pre></td></tr></table></figure>
<p>Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信。docker在后面的18版本又改回来了。</p>
<h1 id="2-使用kubeadm部署Kubernetes"><a href="#2-使用kubeadm部署Kubernetes" class="headerlink" title="2.使用kubeadm部署Kubernetes"></a>2.使用kubeadm部署Kubernetes</h1><h2 id="2-1-安装kubeadm和kubelet"><a href="#2-1-安装kubeadm和kubelet" class="headerlink" title="2.1 安装kubeadm和kubelet"></a>2.1 安装kubeadm和kubelet</h2><p>下面在各节点安装kubeadm和kubelet：<br>使用阿里云k8s 仓，记得禁用check</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</span><br><span class="line">       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y kubelet-1.13.1 kubeadm-1.13.1 kubectl-1.13.1</span><br></pre></td></tr></table></figure>
<p>运行kubelet –help可以看到原来kubelet的绝大多数命令行flag参数都被DEPRECATED了,而官方推荐我们使用–config指定配置文件，并在配置文件中指定原来这些flag所配置的内容。具体内容可以查看这里<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/" target="_blank" rel="noopener">Set Kubelet parameters via a config file</a>。这也是Kubernetes为了支持动态Kubelet配置（Dynamic Kubelet Configuration）才这么做的，参考<a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/" target="_blank" rel="noopener">Reconfigure a Node’s Kubelet in a Live Cluster</a>。</p>
<p>kubelet的配置文件必须是json或yaml格式，具体可查看这里。</p>
<p>Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动。</p>
<p>因为我的机器都开启了swap，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。 之前的Kubernetes版本我们都是通过kubelet的启动参数–fail-swap-on=false去掉这个限制的。前面已经分析了Kubernetes不再推荐使用启动参数，而推荐使用配置文件。 所以这里我们改成配置文件配置的形式。</p>
<p>查看/etc/systemd/system/kubelet.service.d/10-kubeadm.conf，看到了下面的内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Note: This dropin only works with kubeadm and kubelet v1.11+</span></span><br><span class="line">[Service]</span><br><span class="line">Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span><br><span class="line">Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"</span><br><span class="line"><span class="meta">#</span><span class="bash"> This is a file that <span class="string">"kubeadm init"</span> and <span class="string">"kubeadm join"</span> generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically</span></span><br><span class="line">EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env</span><br><span class="line"><span class="meta">#</span><span class="bash"> This is a file that the user can use <span class="keyword">for</span> overrides of the kubelet args as a last resort. Preferably, the user should use</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the .NodeRegistration.KubeletExtraArgs object <span class="keyword">in</span> the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.</span></span><br><span class="line">EnvironmentFile=-/etc/sysconfig/kubelet</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS</span><br></pre></td></tr></table></figure>
<p>上面显示kubeadm部署的kubelet的配置文件–config=/var/lib/kubelet/config.yaml，实际去查看/var/lib/kubelet和这个config.yaml的配置文件都没有被创建。 可以猜想肯定是运行kubeadm初始化集群时会自动生成这个配置文件，而如果我们不关闭Swap的话，第一次初始化集群肯定会失败的。</p>
<p>所以还是老老实实的回到使用kubelet的启动参数–fail-swap-on=false去掉必须关闭Swap的限制。 修改/etc/sysconfig/kubelet，加入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">KUBELET_EXTRA_ARGS=--fail-swap-on=false</span><br></pre></td></tr></table></figure>

<h2 id="2-2-使用kubeadm-init初始化集群"><a href="#2-2-使用kubeadm-init初始化集群" class="headerlink" title="2.2 使用kubeadm init初始化集群"></a>2.2 使用kubeadm init初始化集群</h2><p>在各节点开机启动kubelet服务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl enable kubelet.service</span><br></pre></td></tr></table></figure>

<p>因为默认初始镜像是gcr.io的，如果机器不翻墙是pull 不下来的，所以需要我们提前下载下来，并放到私有仓。</p>
<p>查看镜像版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm config images list</span><br><span class="line"></span><br><span class="line">k8s.gcr.io/kube-apiserver:v1.13.1</span><br><span class="line">k8s.gcr.io/kube-controller-manager:v1.13.1</span><br><span class="line">k8s.gcr.io/kube-scheduler:v1.13.1</span><br><span class="line">k8s.gcr.io/kube-proxy:v1.13.1</span><br><span class="line">k8s.gcr.io/pause:3.1</span><br><span class="line">k8s.gcr.io/etcd:3.2.24</span><br><span class="line">k8s.gcr.io/coredns:1.2.6</span><br></pre></td></tr></table></figure>

<p>生成pull命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm config images list |sed -e 's/^/docker pull /g'</span><br></pre></td></tr></table></figure>

<p>下载镜像，并且上传到私有仓</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker images|grep gcr.io|awk '&#123;print "docker tag ",$1":"$2,$1":"$2&#125;'|sed 's#k8s.gcr.io#docker.foxchan.com/google_containers#2'</span><br><span class="line"></span><br><span class="line">docker tag  k8s.gcr.io/kube-proxy:v1.13.1 docker.foxchan.com/google_containers/kube-proxy:v1.13.1</span><br><span class="line">docker tag  k8s.gcr.io/kube-apiserver:v1.13.1 docker.foxchan.com/google_containers/kube-apiserver:v1.13.1</span><br><span class="line">docker tag  k8s.gcr.io/kube-controller-manager:v1.13.1 docker.foxchan.com/google_containers/kube-controller-manager:v1.13.1</span><br><span class="line">docker tag  k8s.gcr.io/kube-scheduler:v1.13.1 docker.foxchan.com/google_containers/kube-scheduler:v1.13.1</span><br><span class="line">docker tag  k8s.gcr.io/etcd:3.2.24 docker.foxchan.com/google_containers/etcd:3.2.24</span><br><span class="line">docker tag  k8s.gcr.io/coredns:1.2.6 docker.foxchan.com/google_containers/coredns:1.2.6</span><br><span class="line">docker tag  k8s.gcr.io/pause:3.1 docker.foxchan.com/google_containers/pause:3.1</span><br></pre></td></tr></table></figure>
<h3 id="2-2-1通过配置文件安装"><a href="#2-2-1通过配置文件安装" class="headerlink" title="2.2.1通过配置文件安装"></a>2.2.1通过配置文件安装</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm config print init-defaults &gt;kubeadm.conf</span><br></pre></td></tr></table></figure>
<p>将配置文件的imageRepository: 修改为自己的私有仓<br>imageRepository: docker.emarbox.com/google_containers</p>
<p>kubernetesVersion 改为自有版本<br>kubernetesVersion: v1.13.1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm config images list --config kubeadm.conf</span><br><span class="line">kubeadm config images pull --config kubeadm.conf</span><br><span class="line">kubeadm init --config kubeadm.conf</span><br></pre></td></tr></table></figure>
<h3 id="2-2-2-通过参数化安装"><a href="#2-2-2-通过参数化安装" class="headerlink" title="2.2.2 通过参数化安装"></a>2.2.2 通过参数化安装</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm init \</span><br><span class="line">   --kubernetes-version=v1.13.1 \</span><br><span class="line">   --pod-network-cidr=10.244.0.0/16 \</span><br><span class="line">   --apiserver-advertise-address=101.254.242.57 \</span><br><span class="line">   --ignore-preflight-errors=Swap \</span><br><span class="line">   --token-ttl 0 \</span><br><span class="line">   --image-repository docker.emarbox.com/google_containers</span><br></pre></td></tr></table></figure>

<p>–token-ttl 0  初始化的时候指定token不过期<br>如果token过期了，用以下步骤重新生成<br>创建新的token </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm token create</span><br></pre></td></tr></table></figure>
<p>获取ca证书sha256编码hash值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'</span><br></pre></td></tr></table></figure>


<p>因为我们选择flannel作为Pod网络插件，所以上面的命令指定–pod-network-cidr=10.244.0.0/16。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[init] Using Kubernetes version: v1.13.1</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">        [WARNING Swap]: running with swap on is not supported. Please disable swap</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"</span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[certs] Using certificateDir folder "/etc/kubernetes/pki"</span><br><span class="line">[certs] Generating "ca" certificate and key</span><br><span class="line">[certs] Generating "apiserver-kubelet-client" certificate and key</span><br><span class="line">[certs] Generating "apiserver" certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [node1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.61.11]</span><br><span class="line">[certs] Generating "front-proxy-ca" certificate and key</span><br><span class="line">[certs] Generating "front-proxy-client" certificate and key</span><br><span class="line">[certs] Generating "etcd/ca" certificate and key</span><br><span class="line">[certs] Generating "etcd/healthcheck-client" certificate and key</span><br><span class="line">[certs] Generating "etcd/server" certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [node1 localhost] and IPs [192.168.61.11 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating "etcd/peer" certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [node1 localhost] and IPs [192.168.61.11 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating "apiserver-etcd-client" certificate and key</span><br><span class="line">[certs] Generating "sa" key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder "/etc/kubernetes"</span><br><span class="line">[kubeconfig] Writing "admin.conf" kubeconfig file</span><br><span class="line">[kubeconfig] Writing "kubelet.conf" kubeconfig file</span><br><span class="line">[kubeconfig] Writing "controller-manager.conf" kubeconfig file</span><br><span class="line">[kubeconfig] Writing "scheduler.conf" kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder "/etc/kubernetes/manifests"</span><br><span class="line">[control-plane] Creating static Pod manifest for "kube-apiserver"</span><br><span class="line">[control-plane] Creating static Pod manifest for "kube-controller-manager"</span><br><span class="line">[control-plane] Creating static Pod manifest for "kube-scheduler"</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 19.506551 seconds</span><br><span class="line">[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "node1" as an annotation</span><br><span class="line">[mark-control-plane] Marking the node node1 as control-plane by adding the label "node-role.kubernetes.io/master=''"</span><br><span class="line">[mark-control-plane] Marking the node node1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: 702gz5.49zhotgsiyqimwqw</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.1.57:6443 --token pm5kht.s9fh59lop2m34ge9 --discovery-token-ca-cert-hash sha256:9c0f2cc3ab7c2a4bee0532952600befc0e2faf02794c23fa6227870fef18b18a \</span><br><span class="line">  --ignore-preflight-errors=Swap</span><br></pre></td></tr></table></figure>
<p>上面记录了完成的初始化输出的内容，根据输出的内容基本上可以看出手动初始化安装一个Kubernetes集群所需要的关键步骤。</p>
<p>其中有以下关键内容：</p>
<ul>
<li>[kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”</li>
<li>[certificates]生成相关的各种证书</li>
<li>[kubeconfig]生成相关的kubeconfig文件</li>
<li>[bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到</li>
<li>下面的命令是配置常规用户如何使用kubectl访问集群：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
最后给出了将节点加入集群的命令<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join 192.168.1.57:6443 --token pm5kht.s9fh59lop2m34ge9 --discovery-token-ca-cert-hash sha256:9c0f2cc3ab7c2a4bee0532952600befc0e2faf02794c23fa6227870fef18b18a --ignore-preflight-errors=Swap</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>查看一下集群状态：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;"health": "true"&#125;</span><br></pre></td></tr></table></figure>
<p>确认个组件都处于healthy状态。</p>
<p>集群初始化如果遇到问题，可以使用下面的命令进行清理：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf /var/lib/cni/</span><br></pre></td></tr></table></figure>

<p>##2.3 安装Pod Network<br>接下来安装flannel network add-on：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/kubernetes/addons/</span><br><span class="line">cd addons/</span><br><span class="line">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class="line">kubectl apply -f  kube-flannel.yml</span><br><span class="line"></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-amd64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-ppc64le created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-s390x created</span><br></pre></td></tr></table></figure>
<p>这里注意kube-flannel.yml这个文件里的flannel的镜像是0.10.0，quay.io/coreos/flannel:v0.10.0-amd64</p>
<p>如果Node有多个网卡的话，参考flannel issues 39701，目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=<iface-name></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">containers:</span><br><span class="line">      - name: kube-flannel</span><br><span class="line">        image: quay.io/coreos/flannel:v0.10.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - /opt/bin/flanneld</span><br><span class="line">        args:</span><br><span class="line">        - --ip-masq</span><br><span class="line">        - --kube-subnet-mgr</span><br><span class="line">        - --iface=eth1</span><br></pre></td></tr></table></figure>
<p>使用kubectl get pod –all-namespaces -o wide确保所有的Pod都处于Running状态。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE       NAME                                            READY   STATUS    RESTARTS   AGE     IP               NODE             NOMINATED NODE   READINESS GATES</span><br><span class="line">default         curl-66959f6557-9dv5g                           1/1     Running   1          6d3h    10.244.0.4       192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx   nginx-ingress-controller-79b7dccb-6t2nr         1/1     Running   0          3d3h    10.244.2.15      192.168.1.34   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx   nginx-ingress-controller-79b7dccb-r7ljd         1/1     Running   0          3d3h    10.244.1.29      192.168.1.33   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx   nginx-ingress-default-backend-759456dbc-8nrsf   1/1     Running   0          3d3h    10.244.1.28      192.168.1.33   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     coredns-5fcfdd4ccd-9bk7q                        1/1     Running   0          6d20h   10.244.0.2       192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     coredns-5fcfdd4ccd-dzw89                        1/1     Running   0          6d20h   10.244.0.3       192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     etcd-192.168.1.57                               1/1     Running   0          6d20h   192.168.1.57   192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-apiserver-192.168.1.57                     1/1     Running   0          6d20h   192.168.1.57   192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-controller-manager-192.168.1.57            1/1     Running   0          6d20h   192.168.1.57   192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-flannel-ds-amd64-226tp                     1/1     Running   0          6d      192.168.1.33   192.168.1.33   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-flannel-ds-amd64-7r74j                     1/1     Running   0          6d      192.168.1.34   192.168.1.34   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-flannel-ds-amd64-rxzms                     1/1     Running   0          6d3h    192.168.1.57   192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-proxy-5r4mg                                1/1     Running   0          5d23h   192.168.1.34   192.168.1.34   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-proxy-82867                                1/1     Running   0          5d23h   192.168.1.33   192.168.1.33   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-proxy-hqgkw                                1/1     Running   0          5d23h   192.168.1.57   192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     kube-scheduler-192.168.1.57                     1/1     Running   0          6d20h   192.168.1.57   192.168.1.57     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system     tiller-deploy-84bcb9978c-2blph                  1/1     Running   0          3d3h    10.244.2.13      192.168.1.34   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<h2 id="2-4-master-node参与工作负载"><a href="#2-4-master-node参与工作负载" class="headerlink" title="2.4 master node参与工作负载"></a>2.4 master node参与工作负载</h2><p>使用kubeadm初始化的集群，出于安全考虑Pod不会被调度到Master Node上，也就是说Master Node不参与工作负载。这是因为当前的master节点node1被打上了node-role.kubernetes.io/master:NoSchedule的污点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl describe node node1 | grep Taint</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure>
<p>因为这里搭建的是测试环境，去掉这个污点使node1参与工作负载：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes node1 node-role.kubernetes.io/master-</span><br><span class="line">node "node1" untainted</span><br></pre></td></tr></table></figure>
<h2 id="2-5-测试DNS"><a href="#2-5-测试DNS" class="headerlink" title="2.5 测试DNS"></a>2.5 测试DNS</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl run curl --image=radial/busyboxplus:curl -it</span><br><span class="line">kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.</span><br><span class="line">If you don't see a command prompt, try pressing enter.</span><br><span class="line">[ root@curl-5cc7b478b6-r997p:/ ]$</span><br></pre></td></tr></table></figure>
<p>进入后执行nslookup kubernetes.default确认解析正常:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nslookup kubernetes.default</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes.default</span><br><span class="line">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span><br></pre></td></tr></table></figure>
<h2 id="2-6-向Kubernetes集群中添加Node节点"><a href="#2-6-向Kubernetes集群中添加Node节点" class="headerlink" title="2.6 向Kubernetes集群中添加Node节点"></a>2.6 向Kubernetes集群中添加Node节点</h2><p>优先级：<br>/etc/sysconfig/kubelet 参数高于/var/lib/kubelet/config.yaml<br>这里修改上报节点name为ip,修改kubelet</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">KUBELET_EXTRA_ARGS=-<span class="literal">-fail</span><span class="literal">-swap</span><span class="literal">-on</span>=false -<span class="literal">-hostname</span><span class="literal">-override</span>=<span class="number">192.168</span>.<span class="number">1.33</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<p>下面我们将node2这个主机添加到Kubernetes集群中，因为我们同样在node2上的kubelet的启动参数中去掉了必须关闭swap的限制，所以同样需要–ignore-preflight-errors=Swap这个参数。 在node2上执行:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join 192.168.1.57:6443 --token 702gz5.49zhotgsiyqimwqw --discovery-token-ca-cert-hash sha256:2bc50229343849e8021d2aa19d9d314539b40ec7a311b5bb6ca1d3cd10957c2f \</span><br><span class="line"> --ignore-preflight-errors=Swap</span><br><span class="line"></span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">        [WARNING Swap]: running with swap on is not supported. Please disable swap</span><br><span class="line">[discovery] Trying to connect to API Server "192.168.1.57:6443"</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from "https://192.168.61.11:6443"</span><br><span class="line">[discovery] Requesting info from "https://192.168.1.57:6443" again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.1.57:6443"</span><br><span class="line">[discovery] Successfully established connection with API Server "192.168.1.57:6443"</span><br><span class="line">[join] Reading configuration from the cluster...</span><br><span class="line">[join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'</span><br><span class="line">[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"</span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "node2" as an annotation</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run 'kubectl get nodes' on the master to see this node join the cluster.</span><br></pre></td></tr></table></figure>
<p>node2加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">NAME             STATUS   ROLES    AGE     VERSION</span><br><span class="line">192.168.1.33   Ready    edge     6d      v1.13.1</span><br><span class="line">192.168.1.34   Ready    edge     6d      v1.13.1</span><br><span class="line">192.168.1.57     Ready    master   6d20h   v1.13.1</span><br></pre></td></tr></table></figure>
<p>如何从集群中移除Node<br>如果需要从集群中移除node2这个Node执行下面的命令：</p>
<p>在master节点上执行：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl drain node2 --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node node2</span><br></pre></td></tr></table></figure>
<p>在node2上执行：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf /var/lib/cni/</span><br></pre></td></tr></table></figure>
<p>在master上执行：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl delete node node2</span><br></pre></td></tr></table></figure>
<h2 id="2-7-kube-proxy开启ipvs"><a href="#2-7-kube-proxy开启ipvs" class="headerlink" title="2.7 kube-proxy开启ipvs"></a>2.7 kube-proxy开启ipvs</h2><p>修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl edit cm kube-proxy -n kube-system</span><br></pre></td></tr></table></figure>
<p>之后重启各个节点上的kube-proxy pod：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy | awk &#39;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#39;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy</span><br><span class="line">kube-proxy-pf55q                1&#x2F;1     Running   0          9s</span><br><span class="line">kube-proxy-qjnnc                1&#x2F;1     Running   0          14s</span><br><span class="line"></span><br><span class="line">kubectl logs kube-proxy-pf55q -n kube-system</span><br><span class="line">I1208 06:12:23.516444       1 server_others.go:189] Using ipvs Proxier.</span><br><span class="line">W1208 06:12:23.516738       1 proxier.go:365] IPVS scheduler not specified, use rr by default</span><br><span class="line">I1208 06:12:23.516840       1 server_others.go:216] Tearing down inactive rules.</span><br><span class="line">I1208 06:12:23.575222       1 server.go:464] Version: v1.13.0</span><br><span class="line">I1208 06:12:23.585142       1 conntrack.go:52] Setting nf_conntrack_max to 131072</span><br><span class="line">I1208 06:12:23.586203       1 config.go:202] Starting service config controller</span><br><span class="line">I1208 06:12:23.586243       1 controller_utils.go:1027] Waiting for caches to sync for service config controller</span><br><span class="line">I1208 06:12:23.586269       1 config.go:102] Starting endpoints config controller</span><br><span class="line">I1208 06:12:23.586275       1 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller</span><br><span class="line">I1208 06:12:23.686959       1 controller_utils.go:1034] Caches are synced for endpoints config controller</span><br><span class="line">I1208 06:12:23.687056       1 controller_utils.go:1034] Caches are synced for service config controller</span><br></pre></td></tr></table></figure>
<p>日志中打印出了Using ipvs Proxier，说明ipvs模式已经开启。</p>
<p>#3.Kubernetes常用组件部署<br>越来越多的公司和团队开始使用Helm这个Kubernetes的包管理器，我们也将使用Helm安装Kubernetes的常用组件。</p>
<h2 id="3-1-Helm的安装"><a href="#3-1-Helm的安装" class="headerlink" title="3.1 Helm的安装"></a>3.1 Helm的安装</h2><p>Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.12.0版本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;storage.googleapis.com&#x2F;kubernetes-helm&#x2F;helm-v2.12.0-linux-amd64.tar.gz</span><br><span class="line">tar -zxvf helm-v2.12.0-linux-amd64.tar.gz</span><br><span class="line">cd linux-amd64&#x2F;</span><br><span class="line">cp helm &#x2F;usr&#x2F;local&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure>

<p>因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。 详细内容可以查看helm文档中的Role-based Access Control。 这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建rbac-config.yaml文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: tiller</span><br><span class="line">    namespace: kube-system</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create -f rbac-config.yaml</span><br><span class="line">serviceaccount&#x2F;tiller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;tiller created</span><br></pre></td></tr></table></figure>
<p>接下来使用helm部署tiller:<br>使用阿里云镜像，并指定repo为阿里云</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;tiller:v2.12.0 --stable-repo-url https:&#x2F;&#x2F;kubernetes.oss-cn-hangzhou.aliyuncs.com&#x2F;charts</span><br></pre></td></tr></table></figure>
<p>tiller默认被部署在k8s集群中的kube-system这个namespace下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system -l app&#x3D;helm</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">tiller-deploy-c4fd4cd68-dwkhv   1&#x2F;1     Running   0          83s</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.12.0&quot;, GitCommit:&quot;d325d2a9c179b33af1a024cdb5a4472b6288016a&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.12.0&quot;, GitCommit:&quot;d325d2a9c179b33af1a024cdb5a4472b6288016a&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>注意由于某些原因需要网络可以访问gcr.io和kubernetes-charts.storage.googleapis.com，如果无法访问可以通过helm init –service-account tiller –tiller-image <your-docker-registry>/tiller:v2.11.0 –skip-refresh使用私有镜像仓库中的tiller镜像</p>
<h2 id="3-2-使用Helm部署Nginx-Ingress"><a href="#3-2-使用Helm部署Nginx-Ingress" class="headerlink" title="3.2 使用Helm部署Nginx Ingress"></a>3.2 使用Helm部署Nginx Ingress</h2><p>为了便于将集群中的服务暴露到集群外部，从集群外部访问，接下来使用Helm将Nginx Ingress部署到Kubernetes上。 Nginx Ingress Controller被部署在Kubernetes的边缘节点上，关于Kubernetes边缘节点的高可用相关的内容可以查看我前面整理的Bare metal环境下Kubernetes Ingress边缘节点的高可用(基于IPVS)。</p>
<p>我们将node1(192.168.1.33)和node2(192.168.1.34)同时做为边缘节点，打上Label：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl label node 192.168.1.33 node-role.kubernetes.io&#x2F;edge&#x3D;</span><br><span class="line">node&#x2F;node1 labeled</span><br><span class="line"></span><br><span class="line">kubectl label node 192.168.1.34 node-role.kubernetes.io&#x2F;edge&#x3D;</span><br><span class="line">node&#x2F;node2 labeled</span><br><span class="line"></span><br><span class="line">kubectl get node</span><br><span class="line">NAME             STATUS   ROLES    AGE     VERSION</span><br><span class="line">192.168.1.33   Ready    edge     6d      v1.13.1</span><br><span class="line">192.168.1.34   Ready    edge     6d      v1.13.1</span><br><span class="line">192.168.1.57     Ready    master   6d20h   v1.13.1</span><br></pre></td></tr></table></figure>
<p>stable/nginx-ingress chart的值文件ingress-nginx.yaml：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">controller:</span><br><span class="line">  replicaCount: 2</span><br><span class="line">  service:</span><br><span class="line">    externalIPs:</span><br><span class="line">      - 192.168.1.68</span><br><span class="line">  nodeSelector:</span><br><span class="line">    node-role.kubernetes.io/edge: ''</span><br><span class="line">  affinity:</span><br><span class="line">    podAntiAffinity:</span><br><span class="line">        requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        - labelSelector:</span><br><span class="line">            matchExpressions:</span><br><span class="line">            - key: app </span><br><span class="line">              operator: In</span><br><span class="line">              values:</span><br><span class="line">              - nginx-ingress</span><br><span class="line">            - key: component</span><br><span class="line">              operator: In</span><br><span class="line">              values:</span><br><span class="line">              - controller</span><br><span class="line">          topologyKey: kubernetes.io/hostname</span><br><span class="line">  tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io/master</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoSchedule</span><br><span class="line"></span><br><span class="line">defaultBackend:</span><br><span class="line">  nodeSelector:</span><br><span class="line">    node-role.kubernetes.io/edge: ''</span><br><span class="line">  tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io/master</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoSchedule</span><br></pre></td></tr></table></figure>
<p>nginx ingress controller的副本数replicaCount为2，将被调度到node1和node2这两个边缘节点上。externalIPs指定的192.168.1.68为VIP，将绑定到kube-proxy kube-ipvs0网卡上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">helm install stable/nginx-ingress -n nginx-ingress --namespace ingress-nginx  -f ingress-nginx.yaml --set rbac.create=true</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n ingress-nginx -o wide</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE    IP            NODE             NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-ingress-controller-79b7dccb-6t2nr         1/1     Running   0          3d3h   10.244.2.15   192.168.1.34   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ingress-controller-79b7dccb-r7ljd         1/1     Running   0          3d3h   10.244.1.29   192.168.1.33   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ingress-default-backend-759456dbc-8nrsf   1/1     Running   0          3d3h   10.244.1.28   192.168.1.33   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>如果访问<a href="http://192.168.1.68返回default" target="_blank" rel="noopener">http://192.168.1.68返回default</a> backend，则部署完成。</p>
<p>注意：这里的VIP68,只能从k8s集群的node才能访问。</p>
<p>实际测试的结果是无法访问，于是怀疑kube-proxy出了问题，查看kube-proxy的日志，不停的刷下面的log：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">I1208 07:59:28.902970       1 graceful_termination.go:160] Trying to delete rs: 10.104.110.193:80/TCP/10.244.1.5:80</span><br><span class="line">I1208 07:59:28.903037       1 graceful_termination.go:170] Deleting rs: 10.104.110.193:80/TCP/10.244.1.5:80</span><br><span class="line">I1208 07:59:28.903072       1 graceful_termination.go:160] Trying to delete rs: 10.104.110.193:80/TCP/10.244.0.6:80</span><br><span class="line">I1208 07:59:28.903105       1 graceful_termination.go:170] Deleting rs: 10.104.110.193:80/TCP/10.244.0.6:80</span><br><span class="line">I1208 07:59:28.903713       1 graceful_termination.go:160] Trying to delete rs: 192.168.61.10:80/TCP/10.244.1.5:80</span><br><span class="line">I1208 07:59:28.903764       1 graceful_termination.go:170] Deleting rs: 192.168.61.10:80/TCP/10.244.1.5:80</span><br><span class="line">I1208 07:59:28.903798       1 graceful_termination.go:160] Trying to delete rs: 192.168.61.10:80/TCP/10.244.0.6:80</span><br><span class="line">I1208 07:59:28.903824       1 graceful_termination.go:170] Deleting rs: 192.168.61.10:80/TCP/10.244.0.6:80</span><br><span class="line">I1208 07:59:28.904654       1 graceful_termination.go:160] Trying to delete rs: 10.0.2.15:31698/TCP/10.244.0.6:80</span><br><span class="line">I1208 07:59:28.904837       1 graceful_termination.go:170] Deleting rs: 10.0.2.15:31698/TCP/10.244.0.6:80</span><br></pre></td></tr></table></figure>
<p>在Kubernetes的Github上找到了这个ISSUE <a href="https://github.com/kubernetes/kubernetes/issues/71071，大致是最近更新的IPVS" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/71071，大致是最近更新的IPVS</a> proxier mode now support connection based graceful termination.引入了bug，导致Kubernetes的1.11.5、1.12.1~1.12.3、1.13.0都有这个问题，即kube-proxy在ipvs模式下不可用。而官方称在1.11.5、1.12.3、1.13.0中修复了12月4日k8s的特权升级漏洞(CVE-2018-1002105)，如果针对这个漏洞做k8s升级的同学，需要小心，确认是否开启了ipvs，避免由升级引起k8s网络问题。由于我们线上的版本是1.11并且已经启用了ipvs，所以这里我们只能先把线上master node升级到了1.11.5，而kube-proxy还在使用1.11.4的版本。</p>
<p><a href="https://github.com/kubernetes/kubernetes/issues/71071中已经描述有相关PR解决这个问题，后续只能跟踪一下1.11.5、1.12.3、1.13.0之后的小版本了。" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/71071中已经描述有相关PR解决这个问题，后续只能跟踪一下1.11.5、1.12.3、1.13.0之后的小版本了。</a></p>
<p>查看kube-proxy日志，<a href="https://github.com/kubernetes/kubernetes/issues/71071的问题依旧。" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/71071的问题依旧。</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 容器 DNS 设置</title>
    <url>/2020/04/01/2020-04-01-kubernetes%20%E5%AE%B9%E5%99%A8%20DNS%20%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="添加默认配置"><a href="#添加默认配置" class="headerlink" title="添加默认配置"></a>添加默认配置</h1><p>在 kubernetes 中将 DNS 设置配置在 dnsConfig 配置项中， 而 dnsConfig 包含在 PodSpec 配置项中，因此 Pod 内所有容器都共享相同的 Network Namespace 。如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">base/java</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"java -jar /opt/app.jar"</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">dnsConfig:</span></span><br><span class="line">    <span class="attr">nameservers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.</span><span class="string">xxx.xxx.201</span></span><br><span class="line">    <span class="attr">searches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ns1.svc.cluster.local</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">my.dns.search.suffix</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ndots</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">"2"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">edns0</span></span><br></pre></td></tr></table></figure>
<p>通过上述配置创建 Pod 之后，执行 kubectl exec demo cat /etc/resolv.conf 命令即可看到额外的配置项目，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nameserver 10.20.0.2</span><br><span class="line">nameserver 172.xxx.xxx.201</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local ns1.svc.cluster.local my.dns.search.suffix</span><br><span class="line">options ndots:2 edns0</span><br></pre></td></tr></table></figure>
<p>即，在 nameservers 中多了 172.xxx.xxx.201 、 search 中多了 ns1.svc.cluster.local 和 my.dns.search.suffix 两项值，及多了 options ndots:2 edns0 配置。</p>
<p>因此，直接通过 dnsConfig 进行配置后，默认是追加到当前默认配置中。</p>
<a id="more"></a>

<h1 id="DNS-策略"><a href="#DNS-策略" class="headerlink" title="DNS 策略"></a>DNS 策略</h1><p>在 kubernetes 中还提供了 dnsPolicy 决定 Pod 内预设 DNS 配置策略：</p>
<ul>
<li>None 无任何策略</li>
<li>Default 默认</li>
<li>ClusterFirst 集群 DNS 优先</li>
<li>ClusterFirstWithHostNet 集群 DNS 优先，并伴随着使用宿主机网络</li>
</ul>
<h2 id="无策略-None"><a href="#无策略-None" class="headerlink" title="无策略 (None)"></a>无策略 (None)</h2><p>清除 Pod 预设 DNS 配置，当 dnsPolicy 设置成为这个值之后， kubernetes 不会为 Pod 预先加载任何逻辑用于判定得到 DNS 的配置。因此若将 dnsPolicy 设置为 None , 为了避免 Pod 里面没有 DNS 配置，最好通过 dnsConfig 来描述自定义的 DNS 参数。如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">base/java</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"java -jar /opt/app.jar"</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">dnsConfig:</span></span><br><span class="line">    <span class="attr">nameservers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.</span><span class="string">xxx.xxx.201</span></span><br><span class="line">    <span class="attr">searches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ns1.svc.cluster.local</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">my.dns.search.suffix</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ndots</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">"2"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">edns0</span></span><br></pre></td></tr></table></figure>
<p>通过上述配置创建 Pod 之后，执行 kubectl exec demo cat /etc/resolv.conf 命令即可看到额外的配置项目，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nameserver 172.xxx.xxx.201</span><br><span class="line">search ns1.svc.cluster.local my.dns.search.suffix</span><br><span class="line">options ndots:2 edns0</span><br></pre></td></tr></table></figure>
<h2 id="默认预设-Default"><a href="#默认预设-Default" class="headerlink" title="默认预设 (Default)"></a>默认预设 (Default)</h2><p>Pod 里面的 DNS 配置继承了宿主机上的 DNS 配置。即，该 Pod 的 DNS 配置与宿主机完全一致。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">base/java</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"java -jar /opt/app.jar"</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">Default</span></span><br></pre></td></tr></table></figure>
<p>通过 cat /etc/resolv.conf 可查看到宿主机上的配置如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Dynamic resolv.conf(5) file <span class="keyword">for</span> glibc resolver(3) generated by resolvconf(8)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN</span></span><br><span class="line">nameserver 172.xxx.xxx.201</span><br><span class="line">nameserver 114.114.114.114</span><br></pre></td></tr></table></figure>
<p>通过上述配置创建 Pod 之后，执行 kubectl exec demo cat /etc/resolv.conf 命令即可看到额外的配置项目，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nameserver 172.xxx.xxx.201</span><br><span class="line">nameserver 114.114.114.114</span><br></pre></td></tr></table></figure>
<h2 id="集群优先-ClusterFirst"><a href="#集群优先-ClusterFirst" class="headerlink" title="集群优先 (ClusterFirst)"></a>集群优先 (ClusterFirst)</h2><p>与 Default 相反，会预先使用 kube-dns (或 CoreDNS ) 的信息当预设置参数写入到该 Pod 内的DNS配置。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">base/java</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"java -jar /opt/app.jar"</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br></pre></td></tr></table></figure>
<p>通过上述配置创建 Pod 之后，执行 kubectl exec demo cat /etc/resolv.conf 命令即可看到额外的配置项目，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nameserver 10.20.0.2</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>
<p>注 如设置了 hostNetwork = true 时，ClusterFirst 会被强制转化为 Default 。如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">base/java</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"java -jar /opt/app.jar"</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br></pre></td></tr></table></figure>
<p>通过上述配置创建 Pod 之后，执行 kubectl exec demo cat /etc/resolv.conf 命令即可看到额外的配置项目，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nameserver 172.xxx.xxx.201</span><br><span class="line">nameserver 114.114.114.114</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注 设置 hostNetwork = true 之后，会让 Pod 与该节点公用相同的网络空间(网卡/路由等)</p>
</blockquote>
<h2 id="宿主机与-Kubernetes-共存-ClusterFirstWithHostNet"><a href="#宿主机与-Kubernetes-共存-ClusterFirstWithHostNet" class="headerlink" title="宿主机与 Kubernetes 共存 ( ClusterFirstWithHostNet )"></a>宿主机与 Kubernetes 共存 ( ClusterFirstWithHostNet )</h2><p>同时使用 hostNetwork 与 kube-dns 作为 Pod 预设 DNS 配置。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">base/java</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"java -jar /opt/app.jar"</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirstWithHostNet</span></span><br></pre></td></tr></table></figure>
<p>通过上述配置创建 Pod 之后，执行 kubectl exec demo cat /etc/resolv.conf 命令即可看到额外的配置项目，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nameserver 10.20.0.2</span><br><span class="line">search default.svc.k8s.local. svc.k8s.local. k8s.local.</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>


<p>此文章选自：<br>作者：走在成长的道路上<br>链接：<a href="https://www.jianshu.com/p/dbc063e190c9" target="_blank" rel="noopener">https://www.jianshu.com/p/dbc063e190c9</a></p>
<p>​    </p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello HEXO</title>
    <url>/2020/07/23/2020-07-07-hello-world/</url>
    <content><![CDATA[<h1 id="记录下blog服务变化"><a href="#记录下blog服务变化" class="headerlink" title="记录下blog服务变化"></a>记录下blog服务变化</h1><p>迁移blog 到hexo</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>关于kubelet 频繁du, 产生大量io</title>
    <url>/2018/07/16/2018-07-16-kubernetes-kubelet%E9%A2%91%E7%B9%81du-io%E8%BF%87%E9%AB%98/</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>使用iotop查看io情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Total DISK READ :       9.79 M&#x2F;s | Total DISK WRITE :      24.66 K&#x2F;s</span><br><span class="line">Actual DISK READ:       8.21 M&#x2F;s | Actual DISK WRITE:       0.00 B&#x2F;s</span><br><span class="line">  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND                                                      </span><br><span class="line"> 5762 be&#x2F;7 root     1743.49 K&#x2F;s    0.00 B&#x2F;s  0.00 % 95.26 % du -s &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;e~61f203cd85d130b629268078be9c07f4</span><br><span class="line"> 5721 be&#x2F;7 root      750.23 K&#x2F;s    0.00 B&#x2F;s  0.00 % 94.64 % du -s &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;c~dd81ce0684eed62e983301a2bd67e694</span><br><span class="line">   41 be&#x2F;4 root        0.00 B&#x2F;s    0.00 B&#x2F;s  0.00 % 52.87 % [kswapd0]</span><br><span class="line"> 5758 be&#x2F;7 root      200.77 K&#x2F;s    0.00 B&#x2F;s  0.00 % 34.72 % du -s &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;1~803fd9bf781dede9aad4c952530ff38c</span><br><span class="line"> 5761 be&#x2F;7 root      718.53 K&#x2F;s    0.00 B&#x2F;s  0.00 % 24.93 % du -s &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;3~bdb22e8d3cdf465e4a205b1ef72cbd49</span><br><span class="line"> 5759 be&#x2F;7 root      109.19 K&#x2F;s    0.00 B&#x2F;s  0.00 % 17.49 % du -s &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;3~be3fe72d2</span><br></pre></td></tr></table></figure>
<p>发现有好多进程在执行du命令占用了大量的磁盘io，仔细察看是kubelet在调用，kubelet在统计磁盘信息。</p>
<h1 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h1><p>线上环境因为磁盘压力,空间不足的情况出现过大量Pod被驱逐的情形,导致节点上的Pod出现大量被驱逐状态</p>
<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#evicting-end-user-pods" target="_blank" rel="noopener">k8s官方说明</a></p>
<p>k8s默认在磁盘小于10%的时候,就会evict pod,导致线上pod不能正常启动了</p>
<p>官方issue<br><a href="https://github.com/kubernetes/kubernetes/issues/23255" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/23255</a> </p>
<a id="more"></a>

<h1 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h1><ul>
<li>加大磁盘空间，尽量不触发du命令</li>
<li>或者更换ssd，减小io</li>
<li>还有一种是更换docker驱动overlay2，有待验证</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>docker 异常处理集合</title>
    <url>/2020/07/20/2020-07-20-docker-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E9%9B%86%E5%90%88/</url>
    <content><![CDATA[<h1 id="一、kernel-4-6-无法创建container"><a href="#一、kernel-4-6-无法创建container" class="headerlink" title="一、kernel-4.6 无法创建container"></a><strong>一、kernel-4.6 无法创建container</strong></h1><p> 4.0内核,docker官方推荐的是overlay2 做storage driver，但是，xfs文件系统在4.0上与docker不能兼容，需要更改文件系统为ext4。</p>
<p> github链接</p>
<p> <a href="https://github.com/docker/docker/issues/23930" target="_blank" rel="noopener">https://github.com/docker/docker/issues/23930</a></p>
 <a id="more"></a>

<p><strong>二、无法初始化docker</strong></p>
<p> centos6升级到7后，storage driver是overlay的，graph无法初始化，需要把文件系统从ext4换为xfs</p>
<p><strong>三、自定义docker配置(基于centos7)</strong></p>
<p>之前喜欢把/var/lib/docker软链到特定的目录，会有异常bug,比如plugin无法使用，所以抛弃软链</p>
<p>自己有个性化的设置，需要改docker.service</p>
<p>-g 指定docker目录</p>
<p>-s 指定storage driver</p>
<p>-H 指定socket,监听ip/端口号</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sed -i '\/usr\/bin\/dockerd/aEnvironmentFile=/etc/sysconfig/docker'</span><br><span class="line">/usr/lib/systemd/system/docker.service</span><br><span class="line">sed -i s'/dockerd/dockerd $OPTIONS/' /usr/lib/systemd/system/docker.service</span><br><span class="line">echo 'OPTIONS="-g /data/docker -s overlay -H unix:///var/run/docker.sock -H 0.0.0.0:2375"'&gt;/etc/sysconfig/docker</span><br></pre></td></tr></table></figure>





<p><strong>四、device or resource busy</strong></p>
<p>有时候创建、删除、停止容器的时候，会报错如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker start c39206003c7a</span></span><br><span class="line">Error: Cannot start container c39206003c7a: Error getting container c39206003c7ae8992a554a9ac2ea130327fc4af1b2c389656c34baf9a56c84b5 from driver devicemapper: Error mounting '/dev/mapper/docker-253:0-267081-c39206003c7ae8992a554a9ac2ea130327fc4af1b2c389656c34baf9a56c84b5' on '/var/lib/docker/devicemapper/mnt/c39206003c7ae8992a554a9ac2ea130327fc4af1b2c389656c34baf9a56c84b5': device or resource busy</span><br><span class="line">Error: failed to start one or more containers</span><br><span class="line"> </span><br><span class="line"><span class="meta">$</span><span class="bash"> docker rm c39206003c7a</span></span><br><span class="line">Error: Cannot destroy container c39206003c7a: Driver devicemapper failed to remove root filesystem c39206003c7ae8992a554a9ac2ea130327fc4af1b2c389656c34baf9a56c84b5: Error running removeDevice</span><br><span class="line">Error: failed to remove one or more containers</span><br></pre></td></tr></table></figure>

<p> 解决方式2种：</p>
<p>4.1 出现问题一般是storage driver是devicemapper, 更换操作系统用Ubuntu的aufs或者redhat的overlay</p>
<p>4.2 取消挂载</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">umount /var/lib/devicemapper/mnt/ 容器 id</span><br></pre></td></tr></table></figure>



<p>4.3 执行命令</p>
<p>如果状态是Dead的， 先尝试docker rm -f containerId</p>
<p>如果不成功 执行   cat /proc/mounts | grep “mapper/docker” | awk ‘{print $2}’</p>
<h1 id="五、busybox-nslookup-不能使用"><a href="#五、busybox-nslookup-不能使用" class="headerlink" title="五、busybox nslookup 不能使用"></a>五、busybox nslookup 不能使用</h1><p>1.29 开始修改了nslookup的解析方式。 推荐使用1.28</p>
<p>官方issue： <a href="https://github.com/docker-library/busybox/issues/48" target="_blank" rel="noopener">https://github.com/docker-library/busybox/issues/48</a></p>
<p>或者改用 -type=a </p>
<h1 id="六、史诗级bug"><a href="#六、史诗级bug" class="headerlink" title="六、史诗级bug"></a>六、史诗级bug</h1><p>unregister_netdevice: waiting for lo to become free. Usage count = #</p>
<p>issue: <a href="https://github.com/moby/moby/issues/5618" target="_blank" rel="noopener">https://github.com/moby/moby/issues/5618</a></p>
<p>没有解决</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>ss 命令输出详解</title>
    <url>/2020/07/24/2020-07-24-ss%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="ss-命令输出详解"><a href="#ss-命令输出详解" class="headerlink" title="ss 命令输出详解"></a>ss 命令输出详解</h1><p>ss 全名socket statistics，是iproute2中的一员<br>ss已经替代netstat，大热于江湖。但是关于ss命令输出的内容，是什么意思呢？</p>
<a id="more"></a>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[root@test]#</span><span class="bash"> ss -s</span></span><br><span class="line">Total: 26437 (kernel 27730)</span><br><span class="line">TCP:  31961 (estab 25762, closed 6003, orphaned 70, synrecv 0, timewait 5985/0), ports 0</span><br><span class="line"></span><br><span class="line">Transport Total   IP    IPv6</span><br><span class="line">\*     27730   -     -    </span><br><span class="line">RAW    0     0     0    </span><br><span class="line">UDP    21    13    8    </span><br><span class="line">TCP    25958   25073   885   </span><br><span class="line">INET   25979   25086   893   </span><br><span class="line">FRAG   0     0     0</span><br></pre></td></tr></table></figure>


<p>Total: 26437 (kernel 27730) 是什么意思？为什么Total的值小于kernel？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Transport Total   IP    IPv6</span><br><span class="line">\*     27730   -     -</span><br></pre></td></tr></table></figure>

<p>* 又代表什么？</p>
<p>网上全是千篇一律，对于输出格式的文档却没有说明</p>
<p>输出格式共六个字段</p>
<p>netid 它表示套接字类型和传输协议, 当它不明确时：tcp、udp、raw、u str是unix_stream的缩写，u_dgr是unix数据报套接字，nl是netlink，p_raw和p_dgr是raw和datagram数据包套接字</p>
<p>第二列是状态。此处显示套接字状态。这些名称是标准TCP名称，但UNCON除外，它不能用于TCP，但对于其他类型的未连接的套接字是正常的。同样，可以隐藏此列。</p>
<p>然后两列（recv-q和send-q）显示排队等待接收和发送的数据量。</p>
<p>最后两列显示了套接字的本地地址和端口以及它的对等地址（如果套接字已连接）。 </p>
<p>如果给出了选项-o、-e或-p，则不在固定位置显示选项，而是用空格对分隔：选项：值。如果值不是一个数字，它将以值列表的形式显示，括在（…）用逗号分隔</p>
<p>关于* </p>
<p>答：仅是和简单的打开、管理相关的sockets,而不管在内核级别的传输是什么层</p>
<p>关于total 和kernel 的 数量不一致的解释：</p>
<p>答:系统socket 使用后不会立即过期,所以 26437 是所有active sockets 减去过期sockets.。27730 是 连接到kernel sockets，包含（27730-26437）剩余的 还没有被 kernel清理的sockets。<br><a href="http://tweaked.io/guide/kernel/" target="_blank" rel="noopener">http://tweaked.io/guide/kernel/</a><br>这篇关于内核调优的页面很好地概述了内核如何处理套接字的概念，以及如何调整系统以更好地管理内核级别的套接字。</p>
<p>硬翻外国文档</p>
<p><a href="https://superuser.com/questions/885250/what-does-the-output-of-ss-s-mean" target="_blank" rel="noopener">https://superuser.com/questions/885250/what-does-the-output-of-ss-s-mean</a></p>
<p>socket 和port 的区别</p>
<p><a href="https://stackoverflow.com/questions/152457/what-is-the-difference-between-a-port-and-a-socket/152863#152863" target="_blank" rel="noopener">https://stackoverflow.com/questions/152457/what-is-the-difference-between-a-port-and-a-socket/152863#152863</a></p>
<p>ss命令详解</p>
<p><a href="https://www.cyberciti.biz/tips/linux-investigate-sockets-network-connections.html" target="_blank" rel="noopener">https://www.cyberciti.biz/tips/linux-investigate-sockets-network-connections.html</a></p>
<p>Linux创建sockets原理</p>
<p><a href="https://ops.tips/blog/how-linux-creates-sockets/" target="_blank" rel="noopener">https://ops.tips/blog/how-linux-creates-sockets/</a></p>
<p>通过本文知道，kernel 的数值 是取得kernel 全部的sockets,而且不能知道 这些sockets 具体是被哪些占用，就算用不同的namespace，也是一样</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Create a bunch of sockets using our</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> example <span class="keyword">in</span> C</span></span><br><span class="line">./sockets.out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check that we have a bunch of sockets</span></span><br><span class="line">cat /proc/net/sockstat</span><br><span class="line">sockets: used 296</span><br><span class="line">TCP: inuse 5 orphan 0 tw 2 alloc 108 mem 3</span><br><span class="line">UDP: inuse 1 mem 0</span><br><span class="line">UDPLITE: inuse 0</span><br><span class="line">RAW: inuse 0</span><br><span class="line">FRAG: inuse 0 memory 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Create a network namespace</span></span><br><span class="line">ip netns add namespace1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Get into it</span></span><br><span class="line">ip netns exec namespace1 /bin/bash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check how `/proc/net/sockstat` shows the same</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> number of allocated sockets.</span></span><br><span class="line">TCP: inuse 0 orphan 0 tw 0 alloc 108 mem 3</span><br><span class="line">UDP: inuse 0 mem 0</span><br><span class="line">UDPLITE: inuse 0</span><br><span class="line">RAW: inuse 0</span><br><span class="line">FRAG: inuse 0 memory 0</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title>yum error: Failed to initialize NSS library on Centos 7</title>
    <url>/2020/07/24/2020-07-24-yum-error-Failed-to-initialize-NSS-library-on-Centos-7/</url>
    <content><![CDATA[<h1 id="现象："><a href="#现象：" class="headerlink" title="现象："></a>现象：</h1><p>执行rpm或者yum 时候报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">error: Failed to initialize NSS library</span><br><span class="line">There was a problem importing one of the Python modules</span><br><span class="line">required to run yum. The error leading to this problem was:</span><br><span class="line"></span><br><span class="line">  cannot import name ts</span><br><span class="line"></span><br><span class="line">Please install a package which provides this module, or</span><br><span class="line">verify that the module is installed correctly.</span><br><span class="line"></span><br><span class="line">It's possible that the above module doesn't match the</span><br><span class="line">current version of Python, which is:</span><br><span class="line">2.7.5 (default, Aug 4 2017, 00:39:18)</span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)]</span><br><span class="line"></span><br><span class="line">If you cannot solve this problem yourself, please go to</span><br><span class="line">the yum faq at:</span><br><span class="line"> http://yum.baseurl.org/wiki/Faq</span><br></pre></td></tr></table></figure>



<p>这是由于update不完全导致的</p>
<p>查看yum.log日志发现</p>
<p>nss-softokn-freebl 安装一半，或者相关nss依赖包没安装完成。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Apr 16 10:36:23 Installed: jemalloc-devel-3.6.0-1.el7.x86_64</span><br><span class="line">Apr 16 10:36:56 Updated: libgcc-4.8.5-39.el7.x86_64</span><br><span class="line">Apr 16 10:36:56 Updated: nss-softokn-freebl-3.44.0-8.el7_7.x86_64</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h1 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://mirrors.cn99.com/centos/7/os/x86_64/Packages/nss-util-3.44.0-4.el7_7.x86_64.rpm</span><br><span class="line">wget http://mirrors.cn99.com/centos/7/os/x86_64/Packages/nspr-4.21.0-1.el7.x86_64.rpm</span><br><span class="line">wget http://mirrors.cn99.com/centos/7/os/x86_64/Packages/nss-util-3.44.0-4.el7_7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">rpm2cpio nss-util-3.44.0-4.el7_7.x86_64.rpm |cpio -idmv</span><br><span class="line"></span><br><span class="line">cp ./usr/lib64/libnssutil3.so /lib64/ </span><br><span class="line"></span><br><span class="line">rpm2cpio nspr-4.21.0-1.el7.x86_64.rpm | cpio -idmv</span><br><span class="line"></span><br><span class="line">cp -R ./usr/lib64/* /usr/lib64/</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 问题集合</title>
    <url>/2020/08/03/2020-08-03-kubernetes%E9%97%AE%E9%A2%98%E9%9B%86%E5%90%88/</url>
    <content><![CDATA[<h1 id="kubelet问题"><a href="#kubelet问题" class="headerlink" title="kubelet问题"></a>kubelet问题</h1><h2 id="问题一："><a href="#问题一：" class="headerlink" title="问题一："></a>问题一：</h2><p>日志如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Failed to get system container stats for “/system.slice/docker.service”: failed to get cgroup stats for “/system.slice/docker.service”: failed to get container info for “/system.slice/docker.service”: unknown container “/system.slice/docker.service”</span><br></pre></td></tr></table></figure>

<p>处理方式</p>
<p>修改kubelet配置 /etc/sysconfig/kubelet,添加如下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice</span><br></pre></td></tr></table></figure>

<p>原理</p>
<p>大概意思是Flag –cgroup-driver –kubelet-cgroups 驱动已经被禁用，这个参数应该通过kubelet 的配置指定配置文件来配置</p>
<a id="more"></a>

<h2 id="问题二："><a href="#问题二：" class="headerlink" title="问题二："></a>问题二：</h2><p>现象：</p>
<p>kubelet 1.16 在低版本内核无法启动</p>
<p>处理方式</p>
<p>修改kubelet配置 /etc/sysconfig/kubelet,添加如下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--feature-gates SupportPodPidsLimit=false</span><br></pre></td></tr></table></figure>

<p>原理</p>
<p>禁用kubelet新版本特性</p>
<h1 id="coredns问题"><a href="#coredns问题" class="headerlink" title="coredns问题"></a>coredns问题</h1><p>现象：</p>
<p>kubeadm搭建集群，添加网络后，coredns异常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br><span class="line">NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-5dc887ffbb-2s8jd             0/1     Running   0          16h</span><br></pre></td></tr></table></figure>



<p>coredns日志如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pkg/mod/k8s.io/client-go@v0.0.0-20190620085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Namespace: Unauthorized</span><br><span class="line">pkg/mod/k8s.io/client-go@v0.0.0-20190620085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Endpoints: Unauthorized</span><br><span class="line">pkg/mod/k8s.io/client-go@v0.0.0-20190620085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Service: Unauthorized</span><br></pre></td></tr></table></figure>

<p>查看api server日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Unable to authenticate the request due to an error: [invalid bearer token, square/go-jose: error in cryptographic primitive</span><br></pre></td></tr></table></figure>

<p>处理方式：</p>
<p>删除coredns-token和pods后恢复</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl delete secret  -n kube-system coredns-token-6n887</span><br><span class="line">kubectl delete pods coredns-5dc887ffbb-zgff5</span><br></pre></td></tr></table></figure>

<p>原理：</p>
<p>一般发生在你自定义ca，或者升级集群的时候。secret保留的 仍然是依据旧的ca 生成的token，这时需要手动删除token 和pods，让集群根据新的ca重新生成新的token,新的pod 也能使用新的token去访问。</p>
<p>相关git issue：</p>
<p><a href="https://github.com/kubernetes/kubernetes/issues/72026" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/72026</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 安装kube-router</title>
    <url>/2020/08/03/2020-08-03-kubernetes%E5%AE%89%E8%A3%85kube-router/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>网上关于kubernetes网络部署的方案非常多，那为什么还要自己写这么一篇教程呢？因为这篇文章中将介绍一种较为少见的部署方式，使用kuberouter作为kubernetes系统网络组件，直接替换掉kubeproxy、flannel/calico等网络组件。使用这种部署方式的目的，是替换kube-proxy，减少iptables依赖，同时还能提供网络服务。</p>
<a id="more"></a>

<h1 id="替代分析"><a href="#替代分析" class="headerlink" title="替代分析"></a>替代分析</h1><h2 id="Kube-router替代kube-proxy"><a href="#Kube-router替代kube-proxy" class="headerlink" title="Kube-router替代kube-proxy"></a>Kube-router替代kube-proxy</h2><p>kube-proxy的作用主要是用于监听API server中 service 和 endpoint的变化情况，并通过iptables等来为服务配置负载均衡（仅支持TCP和UDP）。kube-proxy 可以直接运行在物理机上，也可以以 static pod 或者 daemonset 的方式运行。</p>
<p>Kube-proxy主要有以下技术特点：</p>
<ul>
<li><p>底层默认使用iptables进行流量的转发</p>
</li>
<li><p>通过监听api server服务中的对象变化，实现service发现功能。</p>
</li>
</ul>
<p>而kuberouter采用了基于相同技术但增加了更多负载均衡策略的IPVS来实现流量转发，服务发现的实现与kube-proxy一致。因此可以直接替代kube-proxy。</p>
<h2 id="Kube-router代替flannel-Calico等网络组件"><a href="#Kube-router代替flannel-Calico等网络组件" class="headerlink" title="Kube-router代替flannel/Calico等网络组件"></a>Kube-router代替flannel/Calico等网络组件</h2><p>kubernetes 对网络的要求是：容器之间（包括同一台主机上的容器，和不同主机的容器）可以互相通信，容器和集群中所有的节点也能直接通信。</p>
<p>在kube-router出现之前，kubernetes长期以来使用第三方模块来负责网络模块配置，其中flannel是其中较为常用的一个，以flannel为例。</p>
<p>flannel在集群中的功能主要有以下两点：</p>
<p>能够给每个 Node 分配互不冲突的网段，意味着集群内每个容器有着互不冲突的 IP 地址；（指定 docker 启动参数的方式指定网段）</p>
<p>能够建立一个覆盖网络，通过这个覆盖网络，能将数据包原封不动传递到目标容器内。（这里传递通常是 UDP 数据包）</p>
<p>在实际运行过程中，每个节点需要部署一个flannel服务，部署成功后会生成一个flannel.0的虚拟设备，同时会运行flanneld的进程，用以链接flannel.0和物理网卡。传输过程中的数据包通过路由规则判断，如果是本网段的访问，就会走 docker0 网桥；如果是跨网段，也就是要跨宿主机访问，就会将数据发往 flannel0。flanneld将数据包封装成底层通信包，协议包括UDP、Vxlan等，这里往往用的是UDP，这个UDP 的packet 会根据 etcd 存放的路由表（一般会缓存到主机内存里）找到目的容器 IP 所承载的宿主机 IP，不借助第三方路由设备的前提下，数据包发送到了宿主机上。此时目标宿主机的 flanneld 进程会对 UDP 包进行解包操作，转化成数据包原本的协议，最终根据主机内部路由流入到目标容器中。</p>
<p>完成这一系列动作有两个核心的技术点：</p>
<p>flannel分配虚拟网卡是基于TUN/TAP虚拟网卡技术。</p>
<p>flannel是三层设备，利用backend对节点内的二层的数据包封包成UDP，再发送到目的节点的flanneld进行解包。</p>
<p>在kuberouter中，这种网络管理机制由CNI（容器网络接口）+BGP协议实现的。</p>
<p>Kube-router不像flannel一样采用子网管理的方式，而是利用kube-conroller-manager来做子网分配，每一个节点会根据cluster CIDR被分配到集群中独一无二的子网ip。Kube-router运行在集群中每个节点运行IBGP，同时自动将所有节点组成网络。所有的集群中的节点在集群中将组成可配置的私有自治网络。Kube-router将会广播pod CIDR，并且保存从同一个主机命名空间学习到的路由。</p>
<p>子网IP的管理是由CNI的brige插件配合IPAM的host-local插件完成。</p>
<h1 id="部署kube-router"><a href="#部署kube-router" class="headerlink" title="部署kube-router"></a>部署kube-router</h1><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><ul>
<li><p>kube-router 能够访问apiserver</p>
<p>这边提供kubeconfig方式</p>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">clusters:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">cluster:</span></span><br><span class="line">    <span class="attr">certificate-authority:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span></span><br><span class="line">    <span class="attr">server:</span> <span class="string">https://k8s.foxchan.com</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">contexts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">context:</span></span><br><span class="line">    <span class="attr">cluster:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">current-context:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">users:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">user:</span></span><br><span class="line">    <span class="attr">tokenFile:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/token</span></span><br></pre></td></tr></table></figure>



<ul>
<li>controller-manager必要配置参数</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--cluster-cidr=10.244.0.0/16 \</span><br><span class="line">--allocate-node-cidrs=true \</span><br></pre></td></tr></table></figure>

<ul>
<li>直接在主机运行需要有ipset命令</li>
<li>以daemonseset 运行需要kube-apiserver 和kubelet 同时开启–allow-privileged=true</li>
<li>如果选择pod-to-pod 网络模式，需要部署CNI插件</li>
</ul>
<p>官方给的简单示例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf</span><br></pre></td></tr></table></figure>

<h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><ul>
<li>使用iptables实现网络策略限制. –run-firewall参数，可透传源IP。</li>
<li>通过bgp实现路由策略.–run-router 参数</li>
<li>通过lvs实现代理策略。 –run-service-proxy</li>
</ul>
<p>–run-firewall, –run-router, –run-service-proxy可以有选择地只启用kube-router所需的功能</p>
<ul>
<li>只提供入口防火墙：–run-firewall=true –run-service-proxy=false –run-router=false</li>
<li>仅仅替换kube-proxy: –run-service-proxy=true –run-firewall=false –run-router=false</li>
</ul>
<p><strong>查看现有集群nodeCIDR划分</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get nodes -o json | jq '.items[] | .spec'</span><br></pre></td></tr></table></figure>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><strong>集群安装时没有选择kube-proxy插件</strong></p>
<blockquote>
<p>从k8s1.14开始，可以在kubeadm init的时候指定组件是否安装</p>
</blockquote>
<p>先要创建kube-proxy configmap，然后修改yaml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl create configmap kube-router --namespace=kube-system --from-file=kubeconfig</span><br><span class="line">wget -o kubeadm-kuberouter-all-features.yaml https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml</span><br></pre></td></tr></table></figure>

<p>修改yaml，去访问之前生成的kubeconfig</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cni-conf-dir</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/cni/net.d</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubeconfig</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/kube-router</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/lib/modules</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cni-conf-dir</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/etc/cni/net.d</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-router-cfg</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-router-cfg</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubeconfig</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-router</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">FileOrCreate</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>

<p><strong>集群已经有kube-proxy</strong></p>
<p>只是进行替换</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml</span><br></pre></td></tr></table></figure>

<p>安装完成后，清理iptables</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system delete ds kube-proxy</span><br><span class="line">docker run --privileged -v /lib/modules:/lib/modules --net=host k8s.gcr.io/kube-proxy-amd64:v1.17.9 kube-proxy --cleanup</span><br></pre></td></tr></table></figure>



<h2 id="命令行汉化"><a href="#命令行汉化" class="headerlink" title="命令行汉化"></a>命令行汉化</h2><p>汉化命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Usage of ./kube-router:</span><br><span class="line">      --advertise-cluster-ip                将该服务的集群IP添加到RIB，以便通告给BGP peers.</span><br><span class="line">      --advertise-external-ip               将服务的外部IP添加到RIB，以便将其通告给BGP peers.</span><br><span class="line">      --cleanup-config                      清理iptables规则，ipvs，ipset配置并退出.</span><br><span class="line">      --cluster-asn uint                    集群节点运行iBGP的ASN编号.</span><br><span class="line">      --cluster-cidr string                 群集中的CIDR范围。它被用来识别pods的范围.</span><br><span class="line">      --config-sync-period duration         apiserver配置同步之间的延迟（例如“5s”，“1m”）。必须大于0.（默认1m0s）</span><br><span class="line">      --enable-overlay                      当enable-overlay设置为true时，IP-in-IP隧道将用于跨不同子网中节点的pod-pod联网。如果设置为false，则不使用隧道，并且路由基础架构预计为不同子网中的节点之间的pod-pod联网路由流量（默认值为true）</span><br><span class="line">      --enable-pod-egress                   从Pod到群集外的SNAT流量。 （默认为true）</span><br><span class="line">      --hairpin-mode                        为每个服务端点添加iptable规则以支持流量管控.</span><br><span class="line">  -h, --help                                打印使用信息.</span><br><span class="line">      --hostname-override string            覆盖节点的NodeName。如果kube-router无法自动确定您的NodeName，请设置此项.</span><br><span class="line">      --iptables-sync-period duration       iptables规则同步之间的延迟（例如'5s'，'1m'）。必须大于0.（默认1m0s）</span><br><span class="line">      --ipvs-sync-period duration           ipvs config同步之间的延迟（例如'5s'，'1m'，'2h22m'）。必须大于0.（默认1m0s）</span><br><span class="line">      --kubeconfig string                   具有授权信息的kubeconfig文件的路径（主位置由主标志设置）。</span><br><span class="line">      --masquerade-all                      SNAT所有流量到群集IP /节点端口。</span><br><span class="line">      --master string                       Kubernetes API服务器的地址（覆盖kubeconfig中的任何值）。</span><br><span class="line">      --nodeport-bindon-all-ip              对于NodePort类型的服务，创建监听节点的所有IP的IPVS服务.</span><br><span class="line">      --nodes-full-mesh                     集群中的每个节点都将建立与其他节点的BGP对等关系。 （默认为true）</span><br><span class="line">      --peer-router-asns uintSlice          集群节点将向其通告集群ip和节点的pid cidr的BGP peers的ASN编号。 （默认[]）</span><br><span class="line">      --peer-router-ips ipSlice             所有节点将对等的外部路由器的IP地址，并通告集群ip和pod cidr。 （默认[]）</span><br><span class="line">      --peer-router-passwords stringSlice   用“--peer-router-ips”定义的BGP peers进行认证的密码。</span><br><span class="line">      --routes-sync-period duration         路线更新与广播之间的延迟（例如“5s”，“1m”，“2h22m”）。必须大于0.（默认1m0s）</span><br><span class="line">      --run-firewall                        启用网络策略 - 设置iptables为pod提供入口防火墙。 （默认为true）</span><br><span class="line">      --run-router                          启用Pod网络 - 通过iBGP发布并学习到Pod的路由。 （默认为true）</span><br><span class="line">      --run-service-proxy                   启用服务代理 - 为Kubernetes服务设置IPVS。 （默认为true）</span><br></pre></td></tr></table></figure>



<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="负载均衡调度算法"><a href="#负载均衡调度算法" class="headerlink" title="负载均衡调度算法"></a>负载均衡调度算法</h2><p>Kube-router使用LVS作为服务代理。 LVS支持丰富的<a href="http://kb.linuxvirtualserver.org/wiki/IPVS#Job_Scheduling_Algorithms" target="_blank" rel="noopener">调度算法</a>。您可以为该服务添加注释以选择一个调度算法。当一个服务没有注释时，默认情况下选择“轮询”调度策略</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">For least connection scheduling use:</span><br><span class="line">kubectl annotate service my-service "kube-router.io/service.scheduler=lc"</span><br><span class="line">For round-robin scheduling use:</span><br><span class="line">kubectl annotate service my-service "kube-router.io/service.scheduler=rr"</span><br><span class="line">For source hashing scheduling use:</span><br><span class="line">kubectl annotate service my-service "kube-router.io/service.scheduler=sh"</span><br><span class="line">For destination hashing scheduling use:</span><br><span class="line">kubectl annotate service my-service "kube-router.io/service.scheduler=dh"</span><br></pre></td></tr></table></figure>

<h2 id="Direct-server-return"><a href="#Direct-server-return" class="headerlink" title="Direct server return"></a>Direct server return</h2><p>请阅读以下博客，了解如何结合使用DSR和“–advertise-external-ip”构建高度可扩展和可用的入口。 <a href="https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/" target="_blank" rel="noopener">https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/</a><br>您可以为每个服务启用DSR（直接服务器返回）功能。当启用的服务端点将直接响应客户端通过签署服务代理。启用DSR时，Kube-router将使用LVS的隧道模式来实现此功能。<br>要启用DSR，您需要使用kube-router.io/service.dsr = tunnel注释来注释服务。例如，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl annotate service my-service "kube-router.io/service.dsr=tunnel"</span><br></pre></td></tr></table></figure>

<p>在当前的实现中，当在服务上应用注释时，DSR将仅适用于外部IP。<br>此外，当使用DSR时，当前的实现不支持端口重新映射。所以你需要使用相同的端口和目标端口的服务<br>你需要在kube-router守护进程清单中启用hostIPC：true和hostPID：true。并且必须将主路径/var/run/docker.sock设置为kube-router的一个volumemount。<br>上述更改需要kube-router输入pod namespace，并在pod中创建ipip隧道，并将外部IP分配给VIP。<br>对于示例清单，请查看启用DSR功能的<a href="https://github.com/cloudnativelabs/kube-router/blob/v1.0.0/daemonset/kubeadm-kuberouter-all-features-dsr.yaml" target="_blank" rel="noopener">manifest</a></p>
<h2 id="暴露服务"><a href="#暴露服务" class="headerlink" title="暴露服务"></a>暴露服务</h2><ul>
<li>svc clusterip</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl expose nginx --target-port=80 --port=80</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc nginx -o template --template=<span class="string">'&#123;&#123;.spec.clusterIP&#125;&#125;'</span></span></span><br><span class="line">10.254.116.179</span><br></pre></td></tr></table></figure>

<p>在每台机器上查看lvs条目</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipvsadm -Ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size&#x3D;4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.254.0.1:443 rr persistent 10800</span><br><span class="line">  -&gt; 172.26.6.1:6443              Masq    1      0          0</span><br><span class="line">TCP  10.254.116.179:80 rr 10800</span><br><span class="line">  -&gt; 10.254.11.2:80               Masq    1      0          0</span><br></pre></td></tr></table></figure>

<p>发现本机SVCIP代理后端真实podip，使用rr算法，通过ip addr s可以看到每添加一个服务node节点上面的kube-dummy-if网卡就会增加一个虚IP</p>
<ul>
<li>svc session-affinity</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl delete svc nginx</span><br><span class="line">kubectl expose deploy nginx --target-port=80 --port=80 --session-affinity=ClientIP</span><br><span class="line">ipvsadm -Ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"><span class="meta">  -&gt;</span><span class="bash"> RemoteAddress:Port           Forward Weight ActiveConn InActConn</span></span><br><span class="line">TCP  10.254.0.1:443 rr persistent 10800</span><br><span class="line"><span class="meta">  -&gt;</span><span class="bash"> 172.26.6.1:6443              Masq    1      0          0</span></span><br><span class="line">TCP  10.254.191.234:80 rr persistent 10800</span><br><span class="line"><span class="meta">  -&gt;</span><span class="bash"> 10.254.11.2:80               Masq    1      0          0</span></span><br><span class="line">我们可以看到 多个persistent，既lvs里面的持久链接</span><br></pre></td></tr></table></figure>

<ul>
<li>svc NodePort</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl delete svc nginx</span><br><span class="line">kubectl expose deploy nginx --target-port=80 --port=80 --type=NodePort</span><br><span class="line">ipvsadm -Ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"><span class="meta">  -&gt;</span><span class="bash"> RemoteAddress:Port           Forward Weight ActiveConn InActConn</span></span><br><span class="line">TCP  172.26.6.3:31117 rr</span><br><span class="line"><span class="meta">  -&gt;</span><span class="bash"> 10.254.11.2:80               Masq    1      0          0</span></span><br><span class="line">TCP  10.254.0.1:443 rr persistent 10800</span><br><span class="line"><span class="meta">  -&gt;</span><span class="bash"> 172.26.6.1:6443              Masq    1      0          0</span></span><br><span class="line">TCP  10.254.102.188:80 rr</span><br><span class="line"><span class="meta">  -&gt;</span><span class="bash"> 10.254.11.2:80               Masq    1      0          0</span></span><br><span class="line">可以看到不仅有虚拟IP条目，还多了对应主机的lvs条目</span><br></pre></td></tr></table></figure>

<ul>
<li>更改算法</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl annotate service nginx "kube-router.io/service.scheduler=dh"</span><br></pre></td></tr></table></figure>

<h2 id="network-policy"><a href="#network-policy" class="headerlink" title="network policy"></a>network policy</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl annotate ns prod "net.beta.kubernetes.io/network-policy=&#123;\"ingress\":&#123;\"isolation\":\"DefaultDeny\"&#125;&#125;"</span><br></pre></td></tr></table></figure>

<p>测试可以看到其他命名空间ping不通该命名空间</p>
<p>查看路由表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ip route s</span><br></pre></td></tr></table></figure>

<p>查看bgp新信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">  kubectl --namespace=kube-system  <span class="built_in">exec</span> -it  kube-router-pk7fs /bin/bash </span></span><br><span class="line"><span class="meta">#</span><span class="bash">  gobgp neighbor -u 172.26.6.3 <span class="comment">#从哪些IP获得更新</span></span></span><br><span class="line">Peer          AS  Up/Down State       |#Received  Accepted</span><br><span class="line">172.26.6.2 64512 01:03:03 Establ      |        1         1</span><br><span class="line"><span class="meta">#</span><span class="bash">  gobgp global rib -u 172.26.6.3 <span class="comment">#global rib相当于路由表</span></span></span><br><span class="line">   Network              Next Hop             AS_PATH              Age        Attrs</span><br><span class="line">*&gt; 10.254.0.0/24        172.26.6.2                                01:03:24   [&#123;Origin: i&#125; &#123;LocalPref: 100&#125;]</span><br><span class="line">*&gt; 10.254.2.0/24        172.26.6.3                                00:00:32   [&#123;Origin: i&#125;]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
</search>
